{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import sys, getopt\n",
    "import csv\n",
    "import pickle\n",
    "import copy\n",
    "import os\n",
    "import math\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "%matplotlib inline  \n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "import umap\n",
    "from sklearn.decomposition import TruncatedSVD as tsvd\n",
    "\n",
    "def nearZeroVarDropAuto(df,thresh=0.99):\n",
    "    vVal=df.var(axis=0).values\n",
    "    cs=pd.Series(vVal).sort_values(ascending=False).cumsum()\n",
    "    remove=cs[cs>cs.values[-1]*thresh].index.values\n",
    "    return df.drop(df.columns[remove],axis=1)\n",
    "\n",
    "%run SodaKick_download_functions.ipynb\n",
    "\n",
    "\n",
    "import sklearn.datasets\n",
    "import sklearn.metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import RepeatedKFold, KFold\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.integration.xgboost import TuneReportCheckpointCallback\n",
    "from functools import partial \n",
    "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "#https://docs.ray.io/en/master/tune/tutorials/tune-xgboost.html\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from ray import tune\n",
    "#from ray.tune import CLIReporter\n",
    "#from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "from hyperopt import hp, tpe, fmin, Trials\n",
    "from hyperopt import STATUS_OK, STATUS_FAIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/quirky-keras-custom-and-asymmetric-loss-functions-for-keras-in-r-a8b5271171fe\n",
    "#weighted asimmetric square error, errors by going below the value (not seeing a goal when it's there) are weighted more\n",
    "\n",
    "def WSE(output, target, a=1.5, b=.5):\n",
    "    loss = torch.mean(a/(a+b)*torch.minimum(torch.zeros(output.shape[1]),output - target)**2+\\\n",
    "                      b/(a+b)*torch.maximum(torch.zeros(output.shape[1]),output - target)**2)      \n",
    "    return loss\n",
    "\n",
    "def WSEl1(output, target, a=1.5, b=.5):\n",
    "    loss = torch.mean(a/(a+b)*torch.abs(torch.minimum(torch.zeros(output.shape[1]),output - target))+\\\n",
    "                      b/(a+b)*torch.abs(torch.maximum(torch.zeros(output.shape[1]),output - target)))      \n",
    "    return loss\n",
    "\n",
    "def WSE2(output, target, a=1.5, b=.5):\n",
    "    loss = np.mean(a/(a+b)*np.minimum(np.zeros(output.shape[0]),output - target)**2+\\\n",
    "                      b/(a+b)*np.maximum(np.zeros(output.shape[0]),output - target)**2)      \n",
    "    return loss\n",
    "\n",
    "def WSEl12(output, target, a=1.5, b=.5):\n",
    "    loss = np.mean(a/(a+b)*np.abs(np.minimum(np.zeros(output.shape[0]),output - target))+\\\n",
    "                      b/(a+b)*np.abs(np.maximum(np.zeros(output.shape[0]),output - target)))      \n",
    "    return loss\n",
    "\n",
    "def log_cosh_loss(y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "    def _log_cosh(x: torch.Tensor) -> torch.Tensor:\n",
    "        return x + torch.nn.functional.softplus(-2. * x) - math.log(2.0)\n",
    "    return torch.mean(_log_cosh(y_pred - y_true))\n",
    "\n",
    "class LogCoshLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(\n",
    "        self, y_pred: torch.Tensor, y_true: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        return log_cosh_loss(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def WSE(predt: np.ndarray, dtrain: xgb.DMatrix) -> Tuple[str, float]:\n",
    "    \n",
    "    target = dtrain.get_label()\n",
    "    predt[predt < -1] = -1 + 1e-6\n",
    "    \n",
    "    a=1.5\n",
    "    b=.5\n",
    "    \n",
    "    elements = a*np.minimum(np.zeros(len(predt)),predt - target)**2+\\\n",
    "                      b*np.maximum(np.zeros(len(predt)),predt - target)**2\n",
    "    \n",
    "    return 'WSE', float(np.sqrt(np.sum(elements) / len(target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_mins(vec):\n",
    "    for i in range(vec.shape[0]):\n",
    "        vec[i][::8]=vec[i][::8]/90\n",
    "\n",
    "def NormalizeData(data):\n",
    "    return (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "\n",
    "def NormalizeMatrix(data):   \n",
    "    for i in range(data.shape[1]):\n",
    "        data[:,i] = NormalizeData(data[:,i])\n",
    "\n",
    "def norm_max(out):\n",
    "    \n",
    "    maxes=[]\n",
    "    for i in range(int(out.shape[1]/8.0)):\n",
    "        maxes.append(out[:,8*int(i):8*(int(i)+1)].max(axis=0))\n",
    "\n",
    "        #maxes.append(out.max(axis=1)[8*int(i):8*(int(i)+1):8])\n",
    "    denominator=np.tile(np.max(maxes,axis=0),int(out.shape[1]/8))\n",
    "    return out/denominator, denominator \n",
    "\n",
    "with open(r'/Users/federico comitani/GitHub/sodakick/data/wainp_220303.pkl', 'rb') as pk:\n",
    "    inp=pickle.load(pk)\n",
    "with open(r'/Users/federico comitani/GitHub/sodakick/data/out_220303.pkl', 'rb') as pk:\n",
    "    out=np.array(pickle.load(pk),dtype=float)\n",
    "    \n",
    "### skipping norm for now since it's already tsvd \n",
    "#NormalizeMatrix(inp)\n",
    "#np.nan_to_num(inp, copy=False)\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "inp = scaler.fit_transform(inp)\n",
    "\n",
    "#normalize_mins(out)\n",
    "out, denominator= norm_max(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "         inp[:25000], out[:25000], test_size=0.2, random_state=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgb(config, col=0):\n",
    "    \n",
    "    #try:\n",
    "        \n",
    "        model = XGBRegressor(**config)\n",
    "        eval_set = [(x_train, y_train[:,col]),(x_test, y_test[:,col])]\n",
    "        model.fit(x_train, y_train[:,col], eval_metric=config['eval_metric'], eval_set=eval_set, early_stopping_rounds = 50, verbose=False)    \n",
    "\n",
    "        evals_result = model.evals_result()\n",
    "\n",
    "        return {'loss': evals_result['validation_1'][config['eval_metric']][-1], 'status': STATUS_OK}\n",
    "    \n",
    "    #except:\n",
    "        \n",
    "    #    return {'loss': np.nan, 'status': STATUS_FAIL}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revert_output(output,multiplier=denominator,lineup=None):\n",
    "\n",
    "    reframe=pd.DataFrame(output.reshape(48,8),\n",
    "                 columns=['minutes','goals','assists','cards_yellow','cards_red','own_goals','goals_against','saves'])\n",
    "    \n",
    "    reframe[reframe<0] = 0\n",
    "    if lineup is not None:\n",
    "        reframe.index=lineup\n",
    "        reframe.drop([x for x in reframe.index if x.startswith('dummy')], axis=0, inplace=True)\n",
    "    \n",
    "    \n",
    "    #reframe['minutes']*=90\n",
    "    reframe=reframe*denominator[:8]\n",
    "    byteamframe=pd.concat([reframe.iloc[:24,:].sum(axis=0),reframe.iloc[24:,:].sum(axis=0)], axis=1).T\n",
    "    \n",
    "    return reframe, byteamframe[byteamframe.columns[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline WSE: 0.039\n",
      "Baseline WSE l1: 0.052\n",
      "Baseline MSE: 0.026\n",
      "Baseline MSE l1: 0.035\n",
      "36.36507936507937\n",
      "24.09365079365079\n",
      "34.76825396825397\n"
     ]
    }
   ],
   "source": [
    "print('Baseline WSE: {:.3f}'.format(WSE2(np.array([0]*out[0].shape[0]),out[0])))\n",
    "print('Baseline WSE l1: {:.3f}'.format(WSEl12(np.array([0]*out[0].shape[0]),out[0])))\n",
    "print('Baseline MSE: {:.3f}'.format(WSE2(np.array([0]*out[0].shape[0]),out[0], a=1, b=1)))\n",
    "print('Baseline MSE l1: {:.3f}'.format(WSEl12(np.array([0]*out[0].shape[0]),out[0], a=1, b=1)))\n",
    "\n",
    "print(np.abs(out[1]-out[10]).sum())\n",
    "print(np.abs(out[50]-out[60]).sum())\n",
    "print(np.abs(out[100]-out[110]).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_hopt(config, num_samples=10, col=0):#, gpus_per_trial=2):\n",
    "    \n",
    "    trials = Trials()\n",
    "    result = fmin(\n",
    "            fn=partial(train_xgb, col=col),\n",
    "            #train_xgb,\n",
    "            space=config,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=num_samples,\n",
    "            trials=trials,\n",
    "            show_progressbar=True),\n",
    "            #early_stop_fn=10,\n",
    "            #trials_save_file=None)\n",
    "    \n",
    "    \n",
    "    return trials\n",
    "    #return best_trained_model\n",
    "    #test_acc = test_accuracy(best_trained_model, device)\n",
    "    #print(\"Best trial test set accuracy: {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature #: 0\n",
      "100%|██████████| 50/50 [07:36<00:00,  9.12s/trial, best loss: 0.136745]\n",
      "Feature #: 1\n",
      "100%|██████████| 50/50 [03:54<00:00,  4.69s/trial, best loss: 0.06655] \n",
      "Feature #: 2\n",
      "100%|██████████| 50/50 [03:35<00:00,  4.31s/trial, best loss: 0.057487]\n",
      "Feature #: 3\n",
      "100%|██████████| 50/50 [04:22<00:00,  5.25s/trial, best loss: 0.133172]\n",
      "Feature #: 4\n",
      "100%|██████████| 50/50 [03:35<00:00,  4.31s/trial, best loss: 0.06808] \n",
      "Feature #: 5\n",
      "100%|██████████| 50/50 [02:45<00:00,  3.31s/trial, best loss: 0.055893]\n",
      "Feature #: 6\n",
      "100%|██████████| 50/50 [03:45<00:00,  4.51s/trial, best loss: 0.048276]\n",
      "Feature #: 7\n",
      "100%|██████████| 50/50 [03:44<00:00,  4.49s/trial, best loss: 0.049079]\n"
     ]
    }
   ],
   "source": [
    "search_space = {\n",
    " \"n_estimators\": 25,\n",
    " \"max_depth\": 2+hp.randint('max_depth', 13),\n",
    " \"min_child_weight\": hp.choice(\"min_child_weight\",[1, 2, 3, 4, 5]),\n",
    " \"subsample\": hp.choice(\"subsample\",np.linspace(.5,.9,5)),\n",
    " \"eta\": hp.choice(\"eta\",[1e-2, 5e-2, 1e-1, 5e-1, 3e-1]),\n",
    " \"colsample_bytree\": hp.choice(\"colsample_bytree\",np.linspace(0.1,.9,9)),\n",
    " \"alpha\": hp.randint(\"alpha\", 5),\n",
    " \"lambda\": hp.randint(\"lambda\", 10),\n",
    " \"gamma\" : hp.choice(\"gamma\",np.linspace(0,.9,10)),\n",
    " \"objective\": \"reg:pseudohubererror\",\n",
    " \"eval_metric\": \"rmse\", \n",
    " \"learning_rate\": 1e-1, \n",
    " }\n",
    "\n",
    "\n",
    "allres=[]\n",
    "for i in range(8):\n",
    "    print('Feature #: '+str(i))\n",
    "    btm = run_hopt(search_space, num_samples=50, col=i)\n",
    "    \n",
    "    results_df=[]\n",
    "    for trial in btm.trials:\n",
    "        results_df.append([trial['result']['loss'],\n",
    "        int(trial['misc']['vals'][\"max_depth\"][0]),\n",
    "        [1, 2, 3, 4, 5][trial['misc']['vals'][\"min_child_weight\"][0]],\n",
    "        np.linspace(.5,.9,5)[trial['misc']['vals'][\"subsample\"][0]],\n",
    "        [1e-2, 5e-2, 1e-1, 5e-1, 3e-1][trial['misc']['vals'][\"eta\"][0]],\n",
    "        np.linspace(0.1,.9,9)[trial['misc']['vals'][\"colsample_bytree\"][0]],\n",
    "        trial['misc']['vals'][\"alpha\"][0],\n",
    "        trial['misc']['vals'][\"lambda\"][0],\n",
    "        np.linspace(0,.9,10)[trial['misc']['vals'][\"gamma\"][0]],                 \n",
    "        ])\n",
    "\n",
    "    allres.append(pd.DataFrame(results_df,columns=['loss',\n",
    "                                            'max_depth',\n",
    "                                            'min_child_weight',\n",
    "                                            'subsample',\n",
    "                                            'eta',\n",
    "                                            'colsample_bytree',\n",
    "                                            'alpha',\n",
    "                                            'lambda',\n",
    "                                            'gamma',\n",
    "                                            ]).sort_values('loss').iloc[0])\n",
    "    allres[-1].name=i\n",
    "    \n",
    "allres=pd.concat(allres,axis=1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cfg=[]\n",
    "for i in range(8):\n",
    "    best_cfg.append(allres.iloc[i].to_dict())\n",
    "    best_cfg[-1][\"objective\"]=\"reg:pseudohubererror\"\n",
    "    best_cfg[-1][\"eval_metric\"]=\"rmse\"\n",
    "    best_cfg[-1][\"learning_rate\"]=1e-1\n",
    "    best_cfg[-1][\"n_estimators\"]=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def runKfold(indata, outdata, num, best_cfg=best_cfg):\n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=True)\n",
    "    kf.get_n_splits(indata)\n",
    "    \n",
    "    tmpparms = best_cfg[num]\n",
    "    tmpparms.pop(\"file\", None)    \n",
    "    tmpparms['n_estimators']=1000\n",
    "    \n",
    "    print(tmpparms)\n",
    "        \n",
    "    ec,losses, errors = [], [], []\n",
    "    rmses = []\n",
    "    results=[]\n",
    "    \n",
    "    for train_index, test_index in kf.split(indata):        \n",
    "        x_train, y_train, x_test, y_test = indata[train_index], outdata[train_index], indata[test_index], outdata[test_index]\n",
    "\n",
    "        train_set = xgb.DMatrix(x_train, label=y_train[:,i])\n",
    "        test_set = xgb.DMatrix(x_test, label=y_test[:,i])\n",
    "    \n",
    "        model=XGBRegressor(**tmpparms)\n",
    "        model.fit(x_train, y_train[:,i],\n",
    "            eval_set = [(x_train, y_train[:,i]),(x_test, y_test[:,i])],\n",
    "            eval_metric = 'rmse',\n",
    "            early_stopping_rounds = 10, verbose=False)\n",
    "        \n",
    "        results.append(model.evals_result())\n",
    "        best_iteration = model.get_booster().best_ntree_limit\n",
    "        \n",
    "        #pred = model.predict(x_test, ntree_limit=best_iteration)\n",
    "        #errors.append(np.mean(pred-y_test[:,i]))\n",
    "\n",
    "        ec.append(model.best_iteration)\n",
    "        losses.append(model.get_booster().best_score)\n",
    "        #rmses.append(results[-1]['validation_1']['rmse'][-1])\n",
    "    \n",
    "    print('Num: {:.3f}+/-{:.3f}'.format(np.mean(ec),np.std(ec)))\n",
    "    print('KFold Result: {:.3f}+/-{:.3f}'.format(np.mean([np.mean(x) for x in losses]),np.std([np.mean(x) for x in losses])))\n",
    "    #print('Error Result: {:.3f}+/-{:.3f}'.format(np.mean([np.mean(x) for x in errors]),np.std([np.mean(x) for x in errors])))\n",
    "    #print('RMSE Result: {:.3f}+/-{:.3f}'.format(np.mean(rmses),np.std(rmses)))\n",
    "\n",
    "    # plot log loss\n",
    "    fig, ax = plt.subplots()\n",
    "    for rs in results:\n",
    "        ax.plot(range(len(rs['validation_0']['rmse'])), rs['validation_0']['rmse'], label='Train', color='blue')\n",
    "        ax.plot(range(len(rs['validation_1']['rmse'])), rs['validation_1']['rmse'], label='Test', color='orange')\n",
    "    plt.show()\n",
    "    \n",
    "    return np.mean([np.mean(x) for x in losses]), np.mean(ec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature #: 0\n",
      "{'loss': 0.136745, 'max_depth': 11.0, 'min_child_weight': 5.0, 'subsample': 0.6, 'eta': 0.5, 'colsample_bytree': 0.6, 'alpha': 0.0, 'lambda': 4.0, 'gamma': 0.4, 'objective': 'reg:pseudohubererror', 'eval_metric': 'rmse', 'learning_rate': 0.1, 'n_estimators': 1000}\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "Invalid Parameter format for max_depth expect int but value='11.0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-c2c1453f4595>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Feature #: '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mrmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunKfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_cfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mrmses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrmse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mepochss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-65-f08bff24d5ab>\u001b[0m in \u001b[0;36mrunKfold\u001b[0;34m(indata, outdata, num, best_cfg)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0meval_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0meval_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'rmse'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             early_stopping_rounds = 10, verbose=False)\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//miniconda3/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//miniconda3/lib/python3.7/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m    601\u001b[0m                               \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_evaluation_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//miniconda3/lib/python3.7/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m    233\u001b[0m                           \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m                           \u001b[0mmaximize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m                           early_stopping_rounds=early_stopping_rounds)\n\u001b[0m\u001b[1;32m    236\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//miniconda3/lib/python3.7/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks, evals_result, maximize, verbose_eval, early_stopping_rounds)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//miniconda3/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1280\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[1;32m   1281\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1282\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1283\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_margin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//miniconda3/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \"\"\"\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mXGBoostError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBGetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mXGBoostError\u001b[0m: Invalid Parameter format for max_depth expect int but value='11.0'"
     ]
    }
   ],
   "source": [
    "rmses, epochss=[],[]\n",
    "for i in range(8):\n",
    "    print('Feature #: '+str(i))\n",
    "    rmse, epochs = runKfold(inp, out, i, best_cfg)\n",
    "    rmses.append(rmse)\n",
    "    epochss.append(epochs)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
