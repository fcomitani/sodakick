{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import sys, getopt\n",
    "import csv\n",
    "import pickle\n",
    "import copy\n",
    "import os\n",
    "import math\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "%matplotlib inline  \n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "import umap\n",
    "from sklearn.decomposition import TruncatedSVD as tsvd\n",
    "\n",
    "def nearZeroVarDropAuto(df,thresh=0.99):\n",
    "    vVal=df.var(axis=0).values\n",
    "    cs=pd.Series(vVal).sort_values(ascending=False).cumsum()\n",
    "    remove=cs[cs>cs.values[-1]*thresh].index.values\n",
    "    return df.drop(df.columns[remove],axis=1)\n",
    "\n",
    "%run SodaKick_download_functions.ipynb\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import SGD, Adagrad, Adam, Adagrad\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from ray import tune\n",
    "#from ray.tune import CLIReporter\n",
    "#from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "from hyperopt import hp, tpe, fmin, Trials\n",
    "from hyperopt import STATUS_OK, STATUS_FAIL\n",
    "from hyperopt.pyll import scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "\n",
    "    \"\"\" Stops the training if loss doesn't improve after a given number of epochs. \"\"\"\n",
    "\n",
    "    def __init__(self, patience=3, epsilon=1e-5, keepBest=True, silent=True):\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): Number of epochs without change before stopping the learning (default 3).\n",
    "            epsilon (float): Minimum change in loss to be considered for early stopping (default 1e-5).\n",
    "            keepBest (bool): Keep track of the best model (memory consuming).\n",
    "        \"\"\"\n",
    "\n",
    "        self.patience = patience\n",
    "        self.epsilon = epsilon\n",
    "        self.counter = 0\n",
    "        \n",
    "        self.bestScore = np.inf\n",
    "     \n",
    "        self.keepBest = keepBest \n",
    "        self.bestModel = None\n",
    "\n",
    "        self.earlyStop = False\n",
    "        self.silent = silent\n",
    "\n",
    "    def __call__(self, loss, model):\n",
    "\n",
    "\n",
    "        \"\"\" Evaluate the loss change between epochs and activates early stop if below epsilon.\n",
    "\n",
    "        Args:\n",
    "            loss (float): current loss.\n",
    "            model (torch model): the current model.\n",
    "        \"\"\"\n",
    "\n",
    "        if loss > self.bestScore - self.epsilon:\n",
    "\n",
    "            self.counter += 1\n",
    "            if not self.silent:\n",
    "                print('EarlyStopping counter: {:d}/{:d}'.format(self.counter,self.patience))\n",
    "\n",
    "            if self.counter >= self.patience:\n",
    "                self.earlyStop = True\n",
    "\n",
    "        else:   \n",
    "\n",
    "            self.counter = 0\n",
    "            self.bestScore = loss\n",
    "\n",
    "            if self.keepBest:\n",
    "                self.bestModel = copy.deepcopy(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class matchesDataset(Dataset):\n",
    "\n",
    "    \"\"\" Extend pytorch Dataset class to include cleaning and training set creation, \"\"\"\n",
    "    \n",
    "    def __init__(self, matches, results):\n",
    "\n",
    "        self.matches = torch.tensor(matches, dtype=torch.float32)\n",
    "        self.results = torch.tensor(results, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        \"\"\" Returns the len of the training sample. \"\"\"\n",
    "        \n",
    "        return len(self.matches)\n",
    "        \n",
    "\n",
    "    def __getitem__(self, index): \n",
    "\n",
    "        \"\"\" Returns a word, a context word and a list of negative words for training for a given index. \n",
    "\n",
    "        Args:\n",
    "            index (int): index for the word selection.\n",
    "\n",
    "        Returns:\n",
    "            (string, string, list of strings): selected word, context word and a randomly drawn list \n",
    "                                               of negative words.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.matches[index], self.results[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/quirky-keras-custom-and-asymmetric-loss-functions-for-keras-in-r-a8b5271171fe\n",
    "#weighted asimmetric square error, errors by going below the value (not seeing a goal when it's there) are weighted more\n",
    "\n",
    "def WSE(output, target, a=1.5, b=.5):\n",
    "    loss = torch.mean(a/(a+b)*torch.minimum(torch.zeros(output.shape[1]),output - target)**2+\\\n",
    "                      b/(a+b)*torch.maximum(torch.zeros(output.shape[1]),output - target)**2)      \n",
    "    return loss\n",
    "\n",
    "def WSEl1(output, target, a=1.5, b=.5):\n",
    "    loss = torch.mean(a/(a+b)*torch.abs(torch.minimum(torch.zeros(output.shape[1]),output - target))+\\\n",
    "                      b/(a+b)*torch.abs(torch.maximum(torch.zeros(output.shape[1]),output - target)))      \n",
    "    return loss\n",
    "\n",
    "def WSE2(output, target, a=1.5, b=.5):\n",
    "    loss = np.mean(a/(a+b)*np.minimum(np.zeros(output.shape[0]),output - target)**2+\\\n",
    "                      b/(a+b)*np.maximum(np.zeros(output.shape[0]),output - target)**2)      \n",
    "    return loss\n",
    "\n",
    "def WSEl12(output, target, a=1.5, b=.5):\n",
    "    loss = np.mean(a/(a+b)*np.abs(np.minimum(np.zeros(output.shape[0]),output - target))+\\\n",
    "                      b/(a+b)*np.abs(np.maximum(np.zeros(output.shape[0]),output - target)))      \n",
    "    return loss\n",
    "\n",
    "def log_cosh_loss(y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "    def _log_cosh(x: torch.Tensor) -> torch.Tensor:\n",
    "        return x + torch.nn.functional.softplus(-2. * x) - math.log(2.0)\n",
    "    return torch.mean(_log_cosh(y_pred - y_true))\n",
    "\n",
    "class LogCoshLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(\n",
    "        self, y_pred: torch.Tensor, y_true: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        return log_cosh_loss(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_mins(vec):\n",
    "    for i in range(vec.shape[0]):\n",
    "        vec[i][::8]=vec[i][::8]/90\n",
    "\n",
    "def NormalizeData(data):\n",
    "    return (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "\n",
    "def NormalizeMatrix(data):   \n",
    "    for i in range(data.shape[1]):\n",
    "        data[:,i] = NormalizeData(data[:,i])\n",
    "\n",
    "def norm_max(out):\n",
    "    \n",
    "    maxes=[]\n",
    "    for i in range(int(out.shape[1]/8.0)):\n",
    "        maxes.append(out[:,8*int(i):8*(int(i)+1)].max(axis=0))\n",
    "\n",
    "        #maxes.append(out.max(axis=1)[8*int(i):8*(int(i)+1):8])\n",
    "    denominator=np.tile(np.max(maxes,axis=0),int(out.shape[1]/8))\n",
    "    return out/denominator, denominator \n",
    "\n",
    "with open(r'/Users/federico comitani/GitHub/sodakick/data/wainp_220303.pkl', 'rb') as pk:\n",
    "    inp=pickle.load(pk)\n",
    "with open(r'/Users/federico comitani/GitHub/sodakick/data/out_220303.pkl', 'rb') as pk:\n",
    "    out=np.array(pickle.load(pk),dtype=float)\n",
    "    \n",
    "### skipping norm for now since it's already tsvd \n",
    "#NormalizeMatrix(inp)\n",
    "#np.nan_to_num(inp, copy=False)\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "inp = scaler.fit_transform(inp)\n",
    "\n",
    "#normalize_mins(out)\n",
    "out, denominator= norm_max(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "         inp[:50000], out[:50000], test_size=0.2, random_state=32)\n",
    "\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0],1,x_train.shape[1])\n",
    "#y_train = y_train.reshape(y_train.shape[0],1,y_train.shape[1])\n",
    "x_test = x_test.reshape(x_test.shape[0],1,x_test.shape[1])\n",
    "#y_test = y_test.reshape(y_test.shape[0],1,y_test.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import floor\n",
    "\n",
    "\n",
    "def conv_out_shape(h_w, kernel_size=1, stride=1, pad=0, dilation=1):\n",
    "\n",
    "\tif isinstance(h_w, list):\n",
    "\t    if type(kernel_size) is not tuple:\n",
    "\t        kernel_size = (kernel_size, kernel_size)\n",
    "\t    h = floor( ((h_w[0] + (2 * pad) - ( dilation * (kernel_size[0] - 1) ) - 1 )/ stride) + 1)\n",
    "\t    w = floor( ((h_w[1] + (2 * pad) - ( dilation * (kernel_size[1] - 1) ) - 1 )/ stride) + 1)\n",
    "\t    return h, w\n",
    "\telse:\n",
    "\t\treturn floor( ((h_w + (2 * pad) - ( dilation * (kernel_size - 1) ) - 1 )/ stride) + 1)\n",
    "\n",
    "\n",
    "class PrintSize(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PrintSize, self).__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        return x\n",
    "\n",
    "class CNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, inp_nodes, out_nodes, activation, final_activation, \n",
    "                 conv_layers, conv_filter_exp, conv_scaling, conv_kernel_size, conv_stride, \n",
    "                 batchnorm, pooling, \n",
    "                 dense_layers, dense_nodes, dense_scaling, dropout_percent):\n",
    "        \n",
    "        super(CNet, self).__init__()    \n",
    "            \n",
    "        self.fc = []\n",
    "        self.cv = []\n",
    "        self.lr_cv = []\n",
    "        self.lr_fc = []\n",
    "        self.pl = []\n",
    "        self.act = activation\n",
    "        self.fact = final_activation\n",
    "        self.cl = conv_layers\n",
    "        self.dl = dense_layers\n",
    "        self.bn_fc = []\n",
    "        self.bn_cv = []\n",
    "        self.dp = []\n",
    "        \n",
    "        self.ps = PrintSize()\n",
    "        \n",
    "        if pooling<=0:\n",
    "            pooling=1\n",
    "            \n",
    "        power=0\n",
    "        for i in range(self.cl):\n",
    "            if i==0:\n",
    "                self.cv.append(nn.Conv1d(in_channels = 1,\n",
    "                                    out_channels = 2**conv_filter_exp,\n",
    "                                    kernel_size = conv_kernel_size,\n",
    "                                    stride = conv_stride))\n",
    "                cos=int(conv_out_shape(inp_nodes,\n",
    "                                    kernel_size = conv_kernel_size,\n",
    "                                    stride = conv_stride)/pooling)\n",
    "            else:\n",
    "                self.cv.append(nn.Conv1d(in_channels =  int(2**conv_filter_exp*(conv_scaling**(power-1))),\n",
    "                                    out_channels = int(2**conv_filter_exp*(conv_scaling**(power))),\n",
    "                                    kernel_size = conv_kernel_size,\n",
    "                                    stride = conv_stride))\n",
    "                cos=int(conv_out_shape(cos,\n",
    "                                    kernel_size = conv_kernel_size,\n",
    "                                    stride = conv_stride)/pooling)\n",
    "                      \n",
    "            self.lr_cv.append(self.act)\n",
    "\n",
    "            if pooling>1:\n",
    "                self.pl.append(nn.MaxPool1d(pooling))\n",
    "            if batchnorm:\n",
    "                self.bn_cv.append(nn.BatchNorm1d(int(cos*2**conv_filter_exp*(conv_scaling**(power)))))\n",
    "                                                            \n",
    "            power+=1\n",
    "            \n",
    "        self.flat = nn.Flatten()        \n",
    "                \n",
    "        cos*=2**conv_filter_exp*(conv_scaling**(power-1))\n",
    "        power=0\n",
    "        for j in range(self.dl): \n",
    "            if j==0:\n",
    "                self.fc.append(nn.Linear(in_features = int(cos),\n",
    "                                    out_features = dense_nodes))\n",
    "            else:\n",
    "                self.fc.append(nn.Linear(in_features = int(dense_nodes*(dense_scaling**(power-1))),\n",
    "                                    out_features = int(dense_nodes*(dense_scaling**power))))\n",
    "     \n",
    "            self.lr_fc.append(nn.LeakyReLU())\n",
    "                                                           \n",
    "            if batchnorm:\n",
    "                self.bn_fc.append(nn.BatchNorm1d(int(dense_nodes*(dense_scaling**power))))\n",
    "            if dropout_percent>0:\n",
    "                self.dp.append(nn.Dropout(dropout_percent))\n",
    "            power+=1\n",
    "            \n",
    "        self.oupt = nn.Linear(int(dense_nodes*(dense_scaling**(power-1))), int(out_nodes))\n",
    "     \n",
    "    def reset_weights(self):\n",
    "\n",
    "        \"\"\" Resets network weights according to chosen distribution. \"\"\"\n",
    "\n",
    "        for f in self.cv:\n",
    "            nn.init.xavier_uniform_(f.weight, gain=np.sqrt(2/(1+0.01**2)))\n",
    "        for f in self.fc:\n",
    "            nn.init.xavier_uniform_(f.weight, gain=np.sqrt(2/(1+0.01**2)))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        \n",
    "        z = x\n",
    "        for i in range(self.cl):        \n",
    "            z = self.cv[i](z)\n",
    "            #z = self.ps(z)\n",
    "            if len(self.pl)>0:\n",
    "                z=self.pl[i](z)\n",
    "            if len(self.bn_cv)>0:\n",
    "                z=self.bn_cv[i](z)\n",
    "            z=self.lr_cv[i](z)     \n",
    "                        \n",
    "        z = self.flat(z)\n",
    "        #z = self.ps(z)\n",
    "                  \n",
    "        for i in range(self.dl):\n",
    "            z = self.fc[i](z)\n",
    "            if len(self.bn_fc)>0:\n",
    "                z=self.bn_fc[i](z)\n",
    "            z=self.lr_fc[i](z)\n",
    "            if len(self.dp)>0:\n",
    "                z=self.dp[i](z)\n",
    "                \n",
    "        if self.fact is not None:\n",
    "            z = self.oupt(self.fact(z))\n",
    "        else:\n",
    "            z = self.oupt(z)\n",
    "        return z\n",
    "    \n",
    "    def clp(self):\n",
    "        with torch.no_grad():\n",
    "            for i in range(self.nl):\n",
    "                self.fc[i].weight.copy_ (self.fc[i].weight.data.clamp(min=0)) \n",
    "            self.oupt.weight.copy_ (self.oupt.weight.data.clamp(min=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config, model=CNet, silent=True, checkpoint_dir=None):\n",
    "    \n",
    "    try:\n",
    "        phases = ['train','val']\n",
    "\n",
    "        #x_train, x_test, y_train, y_test = data[0], data[1], data[2], data[3]\n",
    "\n",
    "        training_set = matchesDataset(x_train, y_train)\n",
    "        trainBatch = torch.utils.data.DataLoader(training_set, batch_size=config['batch_size'], shuffle=config['shuffle'], num_workers=config['num_workers'])\n",
    "\n",
    "        validation_set = matchesDataset(x_test, y_test)\n",
    "        valBatch = torch.utils.data.DataLoader(validation_set, batch_size=config['batch_size'], shuffle=config['shuffle'], num_workers=config['num_workers'])\n",
    "\n",
    "        earlStop = EarlyStopping(patience=config['patience'], keepBest=False)\n",
    "\n",
    "        net = model(config['inp_nodes'], config['out_nodes'], config['activation'], config['final_activation'], \n",
    "                    config['conv_layers'], config['conv_filter_exp'], config['conv_scaling'], \n",
    "                    config['conv_kernel_size'], config['conv_stride'], \n",
    "                    config['batchnorm'], config['pooling'], \n",
    "                    config['dense_layers'], config['dense_nodes'], config['dense_scaling'], \n",
    "                    config['dropout_percent'])\n",
    "            \n",
    "        #net = model(config['num_layers'], config['num_nodes'], config['scaling_factor'], \n",
    "        #            config['num_nodes_out'], config['final_activation'], config['batch_norm'], config['dropout'], config['activation'])\n",
    "\n",
    "        net.reset_weights()\n",
    "\n",
    "        device = \"cpu\"\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda:0\"\n",
    "            if torch.cuda.device_count() > 1:\n",
    "                net = nn.DataParallel(net)\n",
    "        net.to(device)\n",
    "\n",
    "        if checkpoint_dir:\n",
    "            model_state, optimizer_state = torch.load(\n",
    "                os.path.join(checkpoint_dir, \"checkpoint\"))\n",
    "            net.load_state_dict(model_state)\n",
    "            optimizer.load_state_dict(optimizer_state)\n",
    "\n",
    "        if config['optim']=='adam':\n",
    "            optimizer = Adam(net.parameters(), lr=config['lr'])\n",
    "        elif config['optim']=='adagrad':\n",
    "            optimizer = Adagrad(net.parameters(), lr=config['lr'])\n",
    "        else:\n",
    "            print('optim error')\n",
    "            return\n",
    "\n",
    "\n",
    "        losses=[[],[]]\n",
    "        mses=[]\n",
    "        diffs=[]\n",
    "        exit=False\n",
    "\n",
    "        #for epoch in tqdm(range(epochs), desc='Epoch'):\n",
    "        for epoch in range(config['epochs']):\n",
    "\n",
    "            if exit:\n",
    "                break\n",
    "\n",
    "            for phase in phases:\n",
    "                if phase == 'train':\n",
    "                    net.train(True) \n",
    "\n",
    "                    \"\"\" Run the training of the model. \"\"\"    \n",
    "\n",
    "                    losses_batch=[]\n",
    "                    for batchNum, batch in enumerate(trainBatch):\n",
    "\n",
    "                        x = batch[0]\n",
    "                        y = batch[1]\n",
    "\n",
    "                        \"\"\" Move batches to GPU if available. \"\"\"\n",
    "\n",
    "                        if torch.cuda.is_available():\n",
    "                            x = x.cuda()\n",
    "                            y = y.cuda()\n",
    "\n",
    "                        \"\"\" Core of training. \"\"\"\n",
    "\n",
    "                        loss = config['loss_f'](net(x), y)\n",
    "\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        if config['clip']:\n",
    "                            net.clp()\n",
    "\n",
    "                        losses_batch.append(loss)\n",
    "\n",
    "                    \"\"\" Early stop check. \"\"\"\n",
    "\n",
    "                    earlStop(loss, net)\n",
    "                    finalepoch = epoch\n",
    "\n",
    "                    if earlStop.earlyStop:\n",
    "\n",
    "                        if not silent:\n",
    "                            print('Limit loss improvement reached, stopping the training.')\n",
    "\n",
    "                        exit=True \n",
    "\n",
    "                    #losses[0].append(torch.mean(torch.stack(losses_batch)).detach().cpu().numpy())\n",
    "                else:\n",
    "                    net.train(False)\n",
    "                    net.eval()\n",
    "\n",
    "                    val_loss=0\n",
    "                    val_mse=0\n",
    "\n",
    "                    losses_batch=[]\n",
    "                    for batchNum, batch in enumerate(valBatch):\n",
    "\n",
    "                        x = batch[0]\n",
    "                        y = batch[1]\n",
    "\n",
    "                        \"\"\" Move batches to GPU if available. \"\"\"\n",
    "\n",
    "                        if torch.cuda.is_available():\n",
    "                            x = x.cuda()\n",
    "                            y = y.cuda()\n",
    "\n",
    "                        \"\"\" Core of training. \"\"\"\n",
    "\n",
    "                        optimizer.zero_grad()\n",
    "                        output=net(x)\n",
    "                        target=y\n",
    "                        loss = config['loss_f'](output, target)\n",
    "\n",
    "                        #losses_batch.append(loss)\n",
    "                        val_loss+=loss.detach().numpy()\n",
    "                        val_mse+=nn.MSELoss()(output, target).detach().numpy()\n",
    "\n",
    "                    #losses[1].append(torch.mean(torch.stack(losses_batch)).detach().cpu().numpy())\n",
    "\n",
    "                    #with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
    "                    #    path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "                    #    torch.save((net.state_dict(), optimizer.state_dict()), path)\n",
    "\n",
    "                    #tune.report(loss=(val_loss/batchNum), mse=(val_mse/batchNum))\n",
    "                    #tune.report(loss=torch.mean(torch.stack(losses_batch)))\n",
    "\n",
    "        return {'loss': (val_loss/batchNum), 'status': STATUS_OK , 'mse': (val_mse/batchNum)}\n",
    "    \n",
    "    except:\n",
    "        \n",
    "        return {'loss': np.nan, 'status': STATUS_FAIL, 'mse': np.nan}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revert_output(output,multiplier=denominator,lineup=None):\n",
    "\n",
    "    reframe=pd.DataFrame(output.reshape(48,8),\n",
    "                 columns=['minutes','goals','assists','cards_yellow','cards_red','own_goals','goals_against','saves'])\n",
    "    \n",
    "    reframe[reframe<0] = 0\n",
    "    if lineup is not None:\n",
    "        reframe.index=lineup\n",
    "        reframe.drop([x for x in reframe.index if x.startswith('dummy')], axis=0, inplace=True)\n",
    "    \n",
    "    \n",
    "    #reframe['minutes']*=90\n",
    "    reframe=reframe*denominator[:8]\n",
    "    byteamframe=pd.concat([reframe.iloc[:24,:].sum(axis=0),reframe.iloc[24:,:].sum(axis=0)], axis=1).T\n",
    "    \n",
    "    return reframe, byteamframe[byteamframe.columns[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline WSE: 0.039\n",
      "Baseline WSE l1: 0.052\n",
      "Baseline MSE: 0.026\n",
      "Baseline MSE l1: 0.035\n",
      "Baseline logcosh: 0.023\n",
      "36.36507936507937\n",
      "24.09365079365079\n",
      "34.76825396825397\n"
     ]
    }
   ],
   "source": [
    "print('Baseline WSE: {:.3f}'.format(WSE2(np.array([0]*out[0].shape[0]),out[0])))\n",
    "print('Baseline WSE l1: {:.3f}'.format(WSEl12(np.array([0]*out[0].shape[0]),out[0])))\n",
    "print('Baseline MSE: {:.3f}'.format(WSE2(np.array([0]*out[0].shape[0]),out[0], a=1, b=1)))\n",
    "print('Baseline MSE l1: {:.3f}'.format(WSEl12(np.array([0]*out[0].shape[0]),out[0], a=1, b=1)))\n",
    "print('Baseline logcosh: {:.3f}'.format(log_cosh_loss(torch.tensor(np.array([0]*out[0].shape[0])),out[0])))\n",
    "\n",
    "print(np.abs(out[1]-out[10]).sum())\n",
    "print(np.abs(out[50]-out[60]).sum())\n",
    "print(np.abs(out[100]-out[110]).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_hopt(config, num_samples=10):#, gpus_per_trial=2):\n",
    "    \n",
    "    trials = Trials()\n",
    "    result = fmin(\n",
    "            fn=train,\n",
    "            space=config,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=num_samples,\n",
    "            trials=trials,\n",
    "            show_progressbar=True),\n",
    "            #early_stop_fn=10,\n",
    "            #trials_save_file=None)\n",
    "    \n",
    "    \n",
    "    return trials\n",
    "    #return best_trained_model\n",
    "    #test_acc = test_accuracy(best_trained_model, device)\n",
    "    #print(\"Best trial test set accuracy: {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [13:04:33<00:00, 470.74s/trial, best loss: 0.007982965325936675] \n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "        \"inp_nodes\": inp.shape[1],\n",
    "        \"out_nodes\": out.shape[1], \n",
    "        \"batch_size\": 32, #[16, 32, 64, 128]\n",
    "        \"clip\": False, #hp.choice('clip',[True, False]),\n",
    "        \"final_activation\" : None,\n",
    "        \"optim\": 'adam',#hp.choice('optim',['adam', 'adagrad']),\n",
    "        \"lr\": 0.0001,#hp.choice('lr',[0.0001,0.001,.00001]),#hp.loguniform('lr', np.exp(np.log(1e-4)), np.exp(np.log(1e-1))),\n",
    "        \"batchnorm\": False,#hp.choice('batch_norm',[True, False]),\n",
    "        \"dropout_percent\": 0.0,#hp.choice('dropout',[0.0,0.1,0.2,0.3]),#hp.sample_from(lambda _: np.random.uniform(low=0.0, high=.6)),\n",
    "        \"shuffle\": True,\n",
    "        \"num_workers\": 4,\n",
    "        \"patience\": 5,\n",
    "        \"epochs\": 50,\n",
    "    \n",
    "        \"activation\": nn.LeakyReLU(),#nn.SELU(),\n",
    "        \"loss_f\": nn.MSELoss(),#log_cosh_loss,#WSE,#hp.choice('loss_f',[WSE, nn.MSELoss()]), #, nn.L1Loss()\n",
    "    \n",
    "        \"conv_layers\": hp.choice('conv_layers', [1, 2, 3]),\n",
    "        \"conv_filter_exp\": scope.int(hp.quniform('conv_filter_exp', 3, 5, q=1)),\n",
    "        \"conv_scaling\": 2, #hp.uniform('conv_scaling', 2, 4),\n",
    "        \"conv_kernel_size\": 3,#hp.choice('conv_kernel_size', [3, 5]),\n",
    "        \"conv_stride\": 1,#hp.choice('conv_stride', [1, 2]),\n",
    "        \"pooling\": hp.choice('pooling', [1, 2]),\n",
    "        \"dense_layers\": hp.choice('dense_layers', [1, 2]),\n",
    "        \"dense_nodes\": scope.int(hp.quniform('dense_nodes', 100, 400, q=50)),\n",
    "        \"dense_scaling\": hp.uniform('dense_scaling', 0.25, 1),\n",
    "    }\n",
    "    \n",
    "btm = run_hopt(config, num_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df=[]\n",
    "\n",
    "for trial in btm.trials:\n",
    "    results_df.append([trial['result']['loss'],\n",
    "    trial['result']['mse'],\n",
    "                       \n",
    "    [1,2,3][trial['misc']['vals'][\"conv_layers\"][0]],\n",
    "    #trial['misc']['vals'][\"conv_scaling\"][0],\n",
    "                       \n",
    "    trial['misc']['vals'][\"conv_filter_exp\"][0],\n",
    "    #[3,5][trial['misc']['vals'][\"conv_kernel_size\"][0]],\n",
    "    #[1,2][trial['misc']['vals'][\"conv_stride\"][0]],\n",
    "    [1,2][trial['misc']['vals'][\"pooling\"][0]],\n",
    "                       \n",
    "    [1,2][trial['misc']['vals'][\"dense_layers\"][0]],\n",
    "    trial['misc']['vals'][\"dense_scaling\"][0],\n",
    "    trial['misc']['vals'][\"dense_nodes\"][0],\n",
    "                       \n",
    "    ])\n",
    "\n",
    "\n",
    "results_df=pd.DataFrame(results_df,columns=['loss',\n",
    "                                            'mse',\n",
    "                                            'conv_layers',\n",
    "                                            #'conv_scaling',\n",
    "                                            'conv_filter_exp',\n",
    "                                            #'conv_kernel_size',\n",
    "                                            #'conv_stride',\n",
    "                                            'conv_pooling',\n",
    "                                            'dense_layers',\n",
    "                                            'dense_scaling',\n",
    "                                            'dense_nodes',\n",
    "                                            ]).sort_values('loss')\n",
    "\n",
    "results_df.to_hdf(r'/Users/federico comitani/GitHub/sodakick/data/hp_conv_mse_lrelu_220308.h5',key='df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>mse</th>\n",
       "      <th>conv_layers</th>\n",
       "      <th>conv_filter_exp</th>\n",
       "      <th>conv_pooling</th>\n",
       "      <th>dense_layers</th>\n",
       "      <th>dense_scaling</th>\n",
       "      <th>dense_nodes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.007983</td>\n",
       "      <td>0.007983</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.724374</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.008063</td>\n",
       "      <td>0.008063</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.555275</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.008067</td>\n",
       "      <td>0.008067</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.463533</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.008153</td>\n",
       "      <td>0.008153</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.372101</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.008154</td>\n",
       "      <td>0.008154</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.762351</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.008156</td>\n",
       "      <td>0.008156</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.474430</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.008181</td>\n",
       "      <td>0.008181</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.490358</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.008184</td>\n",
       "      <td>0.008184</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.809954</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.008206</td>\n",
       "      <td>0.008206</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.421507</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.008242</td>\n",
       "      <td>0.008242</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.953600</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.008249</td>\n",
       "      <td>0.008249</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.729035</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.008267</td>\n",
       "      <td>0.008267</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.578807</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.008323</td>\n",
       "      <td>0.008323</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.540431</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.008337</td>\n",
       "      <td>0.008337</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.520711</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.008372</td>\n",
       "      <td>0.008372</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.975183</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.008373</td>\n",
       "      <td>0.008373</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.556615</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.008408</td>\n",
       "      <td>0.008408</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.492220</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.008448</td>\n",
       "      <td>0.008448</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.269839</td>\n",
       "      <td>300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.008462</td>\n",
       "      <td>0.008462</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.287746</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.008464</td>\n",
       "      <td>0.008464</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.775545</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.008468</td>\n",
       "      <td>0.008468</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.350598</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.008480</td>\n",
       "      <td>0.008480</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.506814</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.008497</td>\n",
       "      <td>0.008497</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.604309</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.008502</td>\n",
       "      <td>0.008502</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.853024</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.008517</td>\n",
       "      <td>0.008517</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.535283</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.008528</td>\n",
       "      <td>0.008528</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.591948</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.008565</td>\n",
       "      <td>0.008565</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.471508</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.008568</td>\n",
       "      <td>0.008568</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999413</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.008628</td>\n",
       "      <td>0.008628</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.514940</td>\n",
       "      <td>300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.008632</td>\n",
       "      <td>0.008632</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.743670</td>\n",
       "      <td>300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.008633</td>\n",
       "      <td>0.008633</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.351906</td>\n",
       "      <td>250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.008712</td>\n",
       "      <td>0.008712</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.874452</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.008720</td>\n",
       "      <td>0.008720</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.883381</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.008722</td>\n",
       "      <td>0.008722</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.933427</td>\n",
       "      <td>300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.008733</td>\n",
       "      <td>0.008733</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.839592</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.008770</td>\n",
       "      <td>0.008770</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.684170</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.008810</td>\n",
       "      <td>0.008810</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.533787</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.008815</td>\n",
       "      <td>0.008815</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.796797</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.008895</td>\n",
       "      <td>0.008895</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.682501</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.008896</td>\n",
       "      <td>0.008896</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.501449</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.008925</td>\n",
       "      <td>0.008925</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.729500</td>\n",
       "      <td>300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.008939</td>\n",
       "      <td>0.008939</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.990386</td>\n",
       "      <td>300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.008954</td>\n",
       "      <td>0.008954</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.818996</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.009097</td>\n",
       "      <td>0.009097</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.918012</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.009131</td>\n",
       "      <td>0.009131</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.488357</td>\n",
       "      <td>300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.009161</td>\n",
       "      <td>0.009161</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.700751</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.009196</td>\n",
       "      <td>0.009196</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.309719</td>\n",
       "      <td>250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.009228</td>\n",
       "      <td>0.009228</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.635569</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.009347</td>\n",
       "      <td>0.009347</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.603705</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.009370</td>\n",
       "      <td>0.009370</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.394585</td>\n",
       "      <td>250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.009371</td>\n",
       "      <td>0.009371</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.450216</td>\n",
       "      <td>250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.009383</td>\n",
       "      <td>0.009383</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.570044</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.009472</td>\n",
       "      <td>0.009472</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.846665</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.009520</td>\n",
       "      <td>0.009520</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.898950</td>\n",
       "      <td>300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.009549</td>\n",
       "      <td>0.009549</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.820605</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.009572</td>\n",
       "      <td>0.009572</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.321343</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.009716</td>\n",
       "      <td>0.009716</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.662763</td>\n",
       "      <td>250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.009729</td>\n",
       "      <td>0.009729</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.555419</td>\n",
       "      <td>300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.009839</td>\n",
       "      <td>0.009839</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.715981</td>\n",
       "      <td>250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.010285</td>\n",
       "      <td>0.010285</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.616159</td>\n",
       "      <td>300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.010286</td>\n",
       "      <td>0.010286</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.643684</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.010348</td>\n",
       "      <td>0.010348</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.776399</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.010470</td>\n",
       "      <td>0.010470</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.451432</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.011004</td>\n",
       "      <td>0.011004</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.308296</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.011104</td>\n",
       "      <td>0.011104</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.787795</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.011200</td>\n",
       "      <td>0.011200</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.748562</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.011201</td>\n",
       "      <td>0.011201</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.571782</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.011683</td>\n",
       "      <td>0.011683</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.596762</td>\n",
       "      <td>300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.389242</td>\n",
       "      <td>300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.011834</td>\n",
       "      <td>0.011834</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.662184</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.012046</td>\n",
       "      <td>0.012046</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.641706</td>\n",
       "      <td>300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.012070</td>\n",
       "      <td>0.012070</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.907081</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.012699</td>\n",
       "      <td>0.012699</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.653465</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.013042</td>\n",
       "      <td>0.013042</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.465889</td>\n",
       "      <td>250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.013063</td>\n",
       "      <td>0.013063</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.333486</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.013064</td>\n",
       "      <td>0.013064</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.524982</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.013607</td>\n",
       "      <td>0.013607</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.664354</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.013898</td>\n",
       "      <td>0.013898</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.408791</td>\n",
       "      <td>300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.013998</td>\n",
       "      <td>0.013998</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.680899</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.014215</td>\n",
       "      <td>0.014215</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.712561</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.014984</td>\n",
       "      <td>0.014984</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.256146</td>\n",
       "      <td>250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.015553</td>\n",
       "      <td>0.015553</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.746501</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.017472</td>\n",
       "      <td>0.017472</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.655859</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.018802</td>\n",
       "      <td>0.018802</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.433620</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.019287</td>\n",
       "      <td>0.019287</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.458695</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.019462</td>\n",
       "      <td>0.019462</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.617097</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.019573</td>\n",
       "      <td>0.019573</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.702565</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.020386</td>\n",
       "      <td>0.020386</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.808343</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.020504</td>\n",
       "      <td>0.020504</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.600609</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.021019</td>\n",
       "      <td>0.021019</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.556878</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.021864</td>\n",
       "      <td>0.021864</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.587797</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.023759</td>\n",
       "      <td>0.023759</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.918611</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.024186</td>\n",
       "      <td>0.024186</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.630968</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.024416</td>\n",
       "      <td>0.024416</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.691435</td>\n",
       "      <td>300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.025397</td>\n",
       "      <td>0.025397</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.439982</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.026529</td>\n",
       "      <td>0.026529</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.375047</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.027748</td>\n",
       "      <td>0.027748</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.535962</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.028181</td>\n",
       "      <td>0.028181</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.415300</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.028192</td>\n",
       "      <td>0.028192</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.383038</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.028319</td>\n",
       "      <td>0.028319</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.292755</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        loss       mse  conv_layers  conv_filter_exp  conv_pooling  \\\n",
       "27  0.007983  0.007983            1              3.0             2   \n",
       "33  0.008063  0.008063            1              3.0             2   \n",
       "73  0.008067  0.008067            1              3.0             2   \n",
       "89  0.008153  0.008153            1              3.0             2   \n",
       "80  0.008154  0.008154            1              3.0             2   \n",
       "37  0.008156  0.008156            1              3.0             2   \n",
       "96  0.008181  0.008181            1              3.0             2   \n",
       "10  0.008184  0.008184            1              3.0             2   \n",
       "68  0.008206  0.008206            1              3.0             2   \n",
       "49  0.008242  0.008242            1              4.0             2   \n",
       "74  0.008249  0.008249            1              3.0             2   \n",
       "69  0.008267  0.008267            1              3.0             2   \n",
       "85  0.008323  0.008323            1              3.0             2   \n",
       "70  0.008337  0.008337            1              3.0             2   \n",
       "32  0.008372  0.008372            1              3.0             1   \n",
       "72  0.008373  0.008373            1              3.0             2   \n",
       "66  0.008408  0.008408            1              3.0             2   \n",
       "90  0.008448  0.008448            1              3.0             1   \n",
       "65  0.008462  0.008462            1              3.0             2   \n",
       "20  0.008464  0.008464            1              3.0             2   \n",
       "71  0.008468  0.008468            1              3.0             2   \n",
       "82  0.008480  0.008480            1              3.0             2   \n",
       "42  0.008497  0.008497            1              3.0             2   \n",
       "29  0.008502  0.008502            1              3.0             1   \n",
       "57  0.008517  0.008517            1              4.0             1   \n",
       "87  0.008528  0.008528            1              4.0             2   \n",
       "67  0.008565  0.008565            1              3.0             2   \n",
       "22  0.008568  0.008568            1              3.0             2   \n",
       "34  0.008628  0.008628            1              3.0             1   \n",
       "24  0.008632  0.008632            1              3.0             2   \n",
       "53  0.008633  0.008633            1              3.0             2   \n",
       "31  0.008712  0.008712            1              3.0             2   \n",
       "60  0.008720  0.008720            1              3.0             2   \n",
       "28  0.008722  0.008722            1              3.0             2   \n",
       "64  0.008733  0.008733            1              4.0             2   \n",
       "52  0.008770  0.008770            1              5.0             1   \n",
       "91  0.008810  0.008810            1              4.0             2   \n",
       "76  0.008815  0.008815            1              3.0             2   \n",
       "77  0.008895  0.008895            3              3.0             1   \n",
       "41  0.008896  0.008896            1              4.0             1   \n",
       "95  0.008925  0.008925            1              4.0             1   \n",
       "25  0.008939  0.008939            1              3.0             2   \n",
       "63  0.008954  0.008954            1              3.0             2   \n",
       "93  0.009097  0.009097            1              3.0             2   \n",
       "55  0.009131  0.009131            1              3.0             2   \n",
       "97  0.009161  0.009161            1              3.0             2   \n",
       "86  0.009196  0.009196            2              3.0             1   \n",
       "46  0.009228  0.009228            1              5.0             2   \n",
       "7   0.009347  0.009347            2              4.0             2   \n",
       "39  0.009370  0.009370            1              4.0             2   \n",
       "78  0.009371  0.009371            1              3.0             2   \n",
       "84  0.009383  0.009383            1              3.0             2   \n",
       "26  0.009472  0.009472            1              3.0             2   \n",
       "0   0.009520  0.009520            2              4.0             2   \n",
       "23  0.009549  0.009549            1              3.0             2   \n",
       "99  0.009572  0.009572            1              3.0             2   \n",
       "15  0.009716  0.009716            2              4.0             2   \n",
       "35  0.009729  0.009729            1              3.0             2   \n",
       "30  0.009839  0.009839            1              3.0             2   \n",
       "75  0.010285  0.010285            2              3.0             2   \n",
       "79  0.010286  0.010286            1              3.0             2   \n",
       "21  0.010348  0.010348            1              3.0             2   \n",
       "36  0.010470  0.010470            3              3.0             2   \n",
       "5   0.011004  0.011004            2              3.0             2   \n",
       "54  0.011104  0.011104            2              4.0             2   \n",
       "58  0.011200  0.011200            1              3.0             2   \n",
       "38  0.011201  0.011201            3              3.0             1   \n",
       "50  0.011683  0.011683            2              3.0             2   \n",
       "81  0.011719  0.011719            2              3.0             1   \n",
       "56  0.011834  0.011834            3              3.0             2   \n",
       "62  0.012046  0.012046            3              3.0             1   \n",
       "11  0.012070  0.012070            1              4.0             1   \n",
       "98  0.012699  0.012699            2              3.0             2   \n",
       "92  0.013042  0.013042            2              5.0             2   \n",
       "40  0.013063  0.013063            2              3.0             2   \n",
       "2   0.013064  0.013064            3              4.0             1   \n",
       "14  0.013607  0.013607            2              4.0             2   \n",
       "83  0.013898  0.013898            3              4.0             2   \n",
       "94  0.013998  0.013998            3              3.0             2   \n",
       "61  0.014215  0.014215            1              4.0             2   \n",
       "45  0.014984  0.014984            1              3.0             2   \n",
       "44  0.015553  0.015553            2              4.0             1   \n",
       "12  0.017472  0.017472            2              5.0             1   \n",
       "48  0.018802  0.018802            3              3.0             2   \n",
       "17  0.019287  0.019287            3              4.0             2   \n",
       "59  0.019462  0.019462            2              5.0             2   \n",
       "9   0.019573  0.019573            1              4.0             1   \n",
       "1   0.020386  0.020386            2              4.0             2   \n",
       "3   0.020504  0.020504            3              5.0             1   \n",
       "47  0.021019  0.021019            2              4.0             1   \n",
       "4   0.021864  0.021864            3              5.0             1   \n",
       "8   0.023759  0.023759            2              5.0             2   \n",
       "13  0.024186  0.024186            3              4.0             2   \n",
       "43  0.024416  0.024416            3              5.0             2   \n",
       "88  0.025397  0.025397            3              5.0             2   \n",
       "16  0.026529  0.026529            1              5.0             2   \n",
       "6   0.027748  0.027748            3              4.0             2   \n",
       "51  0.028181  0.028181            3              4.0             2   \n",
       "18  0.028192  0.028192            3              4.0             2   \n",
       "19  0.028319  0.028319            2              5.0             2   \n",
       "\n",
       "    dense_layers  dense_scaling  dense_nodes  \n",
       "27             1       0.724374        400.0  \n",
       "33             1       0.555275        400.0  \n",
       "73             1       0.463533        350.0  \n",
       "89             1       0.372101        400.0  \n",
       "80             1       0.762351        350.0  \n",
       "37             1       0.474430        400.0  \n",
       "96             1       0.490358        400.0  \n",
       "10             1       0.809954        400.0  \n",
       "68             1       0.421507        400.0  \n",
       "49             1       0.953600        400.0  \n",
       "74             1       0.729035        350.0  \n",
       "69             1       0.578807        350.0  \n",
       "85             1       0.540431        350.0  \n",
       "70             1       0.520711        400.0  \n",
       "32             1       0.975183        350.0  \n",
       "72             1       0.556615        400.0  \n",
       "66             1       0.492220        400.0  \n",
       "90             1       0.269839        300.0  \n",
       "65             1       0.287746        400.0  \n",
       "20             1       0.775545        400.0  \n",
       "71             1       0.350598        350.0  \n",
       "82             1       0.506814        400.0  \n",
       "42             1       0.604309        400.0  \n",
       "29             1       0.853024        350.0  \n",
       "57             1       0.535283        400.0  \n",
       "87             1       0.591948        400.0  \n",
       "67             1       0.471508        400.0  \n",
       "22             1       0.999413        400.0  \n",
       "34             1       0.514940        300.0  \n",
       "24             1       0.743670        300.0  \n",
       "53             1       0.351906        250.0  \n",
       "31             1       0.874452        400.0  \n",
       "60             1       0.883381        400.0  \n",
       "28             1       0.933427        300.0  \n",
       "64             1       0.839592        350.0  \n",
       "52             1       0.684170        400.0  \n",
       "91             1       0.533787        350.0  \n",
       "76             1       0.796797        400.0  \n",
       "77             1       0.682501        350.0  \n",
       "41             1       0.501449        350.0  \n",
       "95             1       0.729500        300.0  \n",
       "25             1       0.990386        300.0  \n",
       "63             1       0.818996        200.0  \n",
       "93             2       0.918012        400.0  \n",
       "55             1       0.488357        300.0  \n",
       "97             2       0.700751        400.0  \n",
       "86             1       0.309719        250.0  \n",
       "46             1       0.635569        350.0  \n",
       "7              1       0.603705        350.0  \n",
       "39             1       0.394585        250.0  \n",
       "78             1       0.450216        250.0  \n",
       "84             2       0.570044        400.0  \n",
       "26             1       0.846665        350.0  \n",
       "0              1       0.898950        300.0  \n",
       "23             1       0.820605        400.0  \n",
       "99             1       0.321343        350.0  \n",
       "15             1       0.662763        250.0  \n",
       "35             1       0.555419        300.0  \n",
       "30             1       0.715981        250.0  \n",
       "75             1       0.616159        300.0  \n",
       "79             2       0.643684        350.0  \n",
       "21             1       0.776399        400.0  \n",
       "36             1       0.451432        350.0  \n",
       "5              1       0.308296        350.0  \n",
       "54             2       0.787795        400.0  \n",
       "58             2       0.748562        200.0  \n",
       "38             1       0.571782        350.0  \n",
       "50             1       0.596762        300.0  \n",
       "81             1       0.389242        300.0  \n",
       "56             1       0.662184        350.0  \n",
       "62             2       0.641706        300.0  \n",
       "11             2       0.907081        350.0  \n",
       "98             1       0.653465        200.0  \n",
       "92             1       0.465889        250.0  \n",
       "40             2       0.333486        400.0  \n",
       "2              1       0.524982        200.0  \n",
       "14             1       0.664354        150.0  \n",
       "83             1       0.408791        300.0  \n",
       "94             1       0.680899        350.0  \n",
       "61             1       0.712561        100.0  \n",
       "45             2       0.256146        250.0  \n",
       "44             1       0.746501        100.0  \n",
       "12             2       0.655859        200.0  \n",
       "48             2       0.433620        350.0  \n",
       "17             1       0.458695        150.0  \n",
       "59             1       0.617097        150.0  \n",
       "9              2       0.702565        100.0  \n",
       "1              2       0.808343        200.0  \n",
       "3              1       0.600609        150.0  \n",
       "47             1       0.556878        150.0  \n",
       "4              2       0.587797        200.0  \n",
       "8              2       0.918611        150.0  \n",
       "13             1       0.630968        100.0  \n",
       "43             2       0.691435        300.0  \n",
       "88             2       0.439982        350.0  \n",
       "16             2       0.375047        200.0  \n",
       "6              2       0.535962        200.0  \n",
       "51             2       0.415300        350.0  \n",
       "18             2       0.383038        200.0  \n",
       "19             2       0.292755        200.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.sort_values('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [1:52:05<00:00, 134.50s/trial, best loss: 0.007694495580672549]  \n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "        \"inp_nodes\": inp.shape[1],\n",
    "        \"out_nodes\": out.shape[1], \n",
    "        \"batch_size\": 32,#[16, 32, 64, 128]\n",
    "        \"clip\": False, #hp.choice('clip',[True, False]),\n",
    "        \"final_activation\" : None,\n",
    "        \"optim\": 'adam',#hp.choice('optim',['adam', 'adagrad']),\n",
    "        \"lr\": 0.0001,#hp.choice('lr',[0.0001,0.001,.00001]),#hp.loguniform('lr', np.exp(np.log(1e-4)), np.exp(np.log(1e-1))),\n",
    "        \"batchnorm\": False,#hp.choice('batch_norm',[True, False]),\n",
    "        \"dropout_percent\": 0.0,#hp.choice('dropout',[0.0,0.1,0.2,0.3]),#hp.sample_from(lambda _: np.random.uniform(low=0.0, high=.6)),\n",
    "        \"shuffle\": True,\n",
    "        \"num_workers\": 4,\n",
    "        \"patience\": 5,\n",
    "        \"epochs\": 50,\n",
    "    \n",
    "        \"activation\": nn.LeakyReLU(),#nn.SELU(),\n",
    "        \"loss_f\": nn.MSELoss(),#log_cosh_loss,#WSE,#hp.choice('loss_f',[WSE, nn.MSELoss()]), #, nn.L1Loss()\n",
    "    \n",
    "        \"conv_layers\": hp.choice('conv_layers', [1, 2]),\n",
    "        \"conv_filter_exp\": scope.int(hp.quniform('conv_filter_exp', 3, 4, q=1)),\n",
    "        \"conv_scaling\": hp.uniform('conv_scaling', 2, 4),\n",
    "        \"conv_kernel_size\": hp.choice('conv_kernel_size', [3, 5]),\n",
    "        \"conv_stride\": hp.choice('conv_stride', [1, 2]),\n",
    "        \"pooling\": hp.choice('pooling', [2, 3, 4]),\n",
    "        \"dense_layers\": hp.choice('dense_layers', [1, 2]),\n",
    "        \"dense_nodes\": scope.int(hp.quniform('dense_nodes', 300, 500, q=50)),\n",
    "        \"dense_scaling\": .5,#hp.uniform('dense_scaling', 0.25, 1),\n",
    "    }\n",
    "    \n",
    "btm = run_hopt(config, num_samples=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'dense_scaling'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-e2aae3669f3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'misc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'vals'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dense_layers\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mtrial\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'misc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'vals'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dense_scaling\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mtrial\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'misc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'vals'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dense_nodes\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'dense_scaling'"
     ]
    }
   ],
   "source": [
    "results_df=[]\n",
    "\n",
    "for trial in btm.trials:\n",
    "    results_df.append([trial['result']['loss'],\n",
    "    trial['result']['mse'],\n",
    "                       \n",
    "    [1,2][trial['misc']['vals'][\"conv_layers\"][0]],\n",
    "    trial['misc']['vals'][\"conv_scaling\"][0],\n",
    "                       \n",
    "    trial['misc']['vals'][\"conv_filter_exp\"][0],\n",
    "    [3,5][trial['misc']['vals'][\"conv_kernel_size\"][0]],\n",
    "    [1,2][trial['misc']['vals'][\"conv_stride\"][0]],\n",
    "    [2,3,4][trial['misc']['vals'][\"pooling\"][0]],\n",
    "                       \n",
    "    [1,2][trial['misc']['vals'][\"dense_layers\"][0]],\n",
    "    trial['misc']['vals'][\"dense_scaling\"][0],\n",
    "    trial['misc']['vals'][\"dense_nodes\"][0],\n",
    "                       \n",
    "    ])\n",
    "\n",
    "\n",
    "results_df=pd.DataFrame(results_df,columns=['loss',\n",
    "                                            'mse',\n",
    "                                            'conv_layers',\n",
    "                                            'conv_scaling',\n",
    "                                            'conv_filter_exp',\n",
    "                                            'conv_kernel_size',\n",
    "                                            'conv_stride',\n",
    "                                            'conv_pooling',\n",
    "                                            'dense_layers',\n",
    "                                            'dense_scaling',\n",
    "                                            'dense_nodes',\n",
    "                                            ]).sort_values('loss')\n",
    "\n",
    "#results_df.to_hdf(r'/Users/federico comitani/GitHub/sodakick/data/hp_conv_mse_lrelu_220308.h5',key='df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df=[]\n",
    "\n",
    "for trial in btm.trials:\n",
    "    results_df.append([trial['result']['loss'],\n",
    "    trial['result']['mse'],\n",
    "                       \n",
    "    [1,2][trial['misc']['vals'][\"conv_layers\"][0]],\n",
    "    trial['misc']['vals'][\"conv_scaling\"][0],\n",
    "                       \n",
    "    trial['misc']['vals'][\"conv_filter_exp\"][0],\n",
    "    [3,5][trial['misc']['vals'][\"conv_kernel_size\"][0]],\n",
    "    [1,2][trial['misc']['vals'][\"conv_stride\"][0]],\n",
    "    [2,3,4][trial['misc']['vals'][\"pooling\"][0]],\n",
    "                       \n",
    "    [1,2][trial['misc']['vals'][\"dense_layers\"][0]],\n",
    "    #trial['misc']['vals'][\"dense_scaling\"][0],\n",
    "    trial['misc']['vals'][\"dense_nodes\"][0],\n",
    "                       \n",
    "    ])\n",
    "\n",
    "\n",
    "results_df=pd.DataFrame(results_df,columns=['loss',\n",
    "                                            'mse',\n",
    "                                            'conv_layers',\n",
    "                                            'conv_scaling',\n",
    "                                            'conv_filter_exp',\n",
    "                                            'conv_kernel_size',\n",
    "                                            'conv_stride',\n",
    "                                            'conv_pooling',\n",
    "                                            'dense_layers',\n",
    "                                            #'dense_scaling',\n",
    "                                            'dense_nodes',\n",
    "                                            ]).sort_values('loss')\n",
    "\n",
    "results_df.to_hdf(r'/Users/federico comitani/GitHub/sodakick/data/hp_conv_mse_lrelu_220309.h5',key='df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>mse</th>\n",
       "      <th>conv_layers</th>\n",
       "      <th>conv_scaling</th>\n",
       "      <th>conv_filter_exp</th>\n",
       "      <th>conv_kernel_size</th>\n",
       "      <th>conv_stride</th>\n",
       "      <th>conv_pooling</th>\n",
       "      <th>dense_layers</th>\n",
       "      <th>dense_nodes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.008155</td>\n",
       "      <td>0.008155</td>\n",
       "      <td>1</td>\n",
       "      <td>3.560791</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>450.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.008187</td>\n",
       "      <td>0.008187</td>\n",
       "      <td>1</td>\n",
       "      <td>3.171815</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.008385</td>\n",
       "      <td>0.008385</td>\n",
       "      <td>2</td>\n",
       "      <td>3.877083</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.008426</td>\n",
       "      <td>0.008426</td>\n",
       "      <td>1</td>\n",
       "      <td>3.241657</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.008441</td>\n",
       "      <td>0.008441</td>\n",
       "      <td>1</td>\n",
       "      <td>2.190875</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>450.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.008454</td>\n",
       "      <td>0.008454</td>\n",
       "      <td>1</td>\n",
       "      <td>2.483373</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.008656</td>\n",
       "      <td>0.008656</td>\n",
       "      <td>1</td>\n",
       "      <td>3.340688</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.009257</td>\n",
       "      <td>0.009257</td>\n",
       "      <td>1</td>\n",
       "      <td>2.195715</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>450.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.009311</td>\n",
       "      <td>0.009311</td>\n",
       "      <td>1</td>\n",
       "      <td>2.776706</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>450.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.009389</td>\n",
       "      <td>0.009389</td>\n",
       "      <td>1</td>\n",
       "      <td>2.007851</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.009406</td>\n",
       "      <td>0.009406</td>\n",
       "      <td>2</td>\n",
       "      <td>3.375809</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.009459</td>\n",
       "      <td>0.009459</td>\n",
       "      <td>1</td>\n",
       "      <td>3.034531</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>450.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.009654</td>\n",
       "      <td>0.009654</td>\n",
       "      <td>1</td>\n",
       "      <td>2.411362</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>450.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.009663</td>\n",
       "      <td>0.009663</td>\n",
       "      <td>1</td>\n",
       "      <td>2.302043</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.009764</td>\n",
       "      <td>0.009764</td>\n",
       "      <td>1</td>\n",
       "      <td>2.536534</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.009883</td>\n",
       "      <td>0.009883</td>\n",
       "      <td>1</td>\n",
       "      <td>3.219700</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.010162</td>\n",
       "      <td>0.010162</td>\n",
       "      <td>1</td>\n",
       "      <td>2.425661</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.010662</td>\n",
       "      <td>0.010662</td>\n",
       "      <td>1</td>\n",
       "      <td>3.704827</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.010704</td>\n",
       "      <td>0.010704</td>\n",
       "      <td>1</td>\n",
       "      <td>3.151312</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.010712</td>\n",
       "      <td>0.010712</td>\n",
       "      <td>1</td>\n",
       "      <td>3.055861</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.010817</td>\n",
       "      <td>0.010817</td>\n",
       "      <td>1</td>\n",
       "      <td>3.329097</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2.108273</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>3.671860</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>3.614779</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>450.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2.421561</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>3.258278</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2.458366</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2.761259</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>450.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>3.574750</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>450.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2.706420</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>3.550690</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2.171490</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>3.277645</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>3.991533</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>3.559307</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>3.156904</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2.175175</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>450.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2.696972</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>3.647958</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2.614750</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2.224338</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>450.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>3.389485</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>450.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>3.656383</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>3.997190</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2.773066</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>3.601767</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2.297967</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>3.171865</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>3.928604</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>3.586173</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        loss       mse  conv_layers  conv_scaling  conv_filter_exp  \\\n",
       "46  0.008155  0.008155            1      3.560791              3.0   \n",
       "4   0.008187  0.008187            1      3.171815              3.0   \n",
       "48  0.008385  0.008385            2      3.877083              3.0   \n",
       "19  0.008426  0.008426            1      3.241657              3.0   \n",
       "21  0.008441  0.008441            1      2.190875              4.0   \n",
       "1   0.008454  0.008454            1      2.483373              4.0   \n",
       "3   0.008656  0.008656            1      3.340688              4.0   \n",
       "20  0.009257  0.009257            1      2.195715              4.0   \n",
       "41  0.009311  0.009311            1      2.776706              4.0   \n",
       "11  0.009389  0.009389            1      2.007851              3.0   \n",
       "32  0.009406  0.009406            2      3.375809              4.0   \n",
       "2   0.009459  0.009459            1      3.034531              3.0   \n",
       "6   0.009654  0.009654            1      2.411362              3.0   \n",
       "9   0.009663  0.009663            1      2.302043              3.0   \n",
       "39  0.009764  0.009764            1      2.536534              3.0   \n",
       "35  0.009883  0.009883            1      3.219700              4.0   \n",
       "5   0.010162  0.010162            1      2.425661              4.0   \n",
       "33  0.010662  0.010662            1      3.704827              4.0   \n",
       "24  0.010704  0.010704            1      3.151312              4.0   \n",
       "14  0.010712  0.010712            1      3.055861              4.0   \n",
       "45  0.010817  0.010817            1      3.329097              4.0   \n",
       "0        NaN       NaN            2      2.108273              4.0   \n",
       "7        NaN       NaN            2      3.671860              4.0   \n",
       "8        NaN       NaN            2      3.614779              4.0   \n",
       "10       NaN       NaN            2      2.421561              3.0   \n",
       "12       NaN       NaN            2      3.258278              3.0   \n",
       "13       NaN       NaN            2      2.458366              4.0   \n",
       "15       NaN       NaN            2      2.761259              3.0   \n",
       "16       NaN       NaN            2      3.574750              3.0   \n",
       "17       NaN       NaN            2      2.706420              4.0   \n",
       "18       NaN       NaN            2      3.550690              4.0   \n",
       "22       NaN       NaN            2      2.171490              3.0   \n",
       "23       NaN       NaN            2      3.277645              4.0   \n",
       "25       NaN       NaN            2      3.991533              3.0   \n",
       "26       NaN       NaN            2      3.559307              3.0   \n",
       "27       NaN       NaN            2      3.156904              3.0   \n",
       "28       NaN       NaN            2      2.175175              3.0   \n",
       "29       NaN       NaN            2      2.696972              4.0   \n",
       "30       NaN       NaN            2      3.647958              3.0   \n",
       "31       NaN       NaN            2      2.614750              4.0   \n",
       "34       NaN       NaN            2      2.224338              3.0   \n",
       "36       NaN       NaN            2      3.389485              3.0   \n",
       "37       NaN       NaN            2      3.656383              3.0   \n",
       "38       NaN       NaN            2      3.997190              3.0   \n",
       "40       NaN       NaN            2      2.773066              3.0   \n",
       "42       NaN       NaN            2      3.601767              4.0   \n",
       "43       NaN       NaN            2      2.297967              3.0   \n",
       "44       NaN       NaN            2      3.171865              4.0   \n",
       "47       NaN       NaN            2      3.928604              3.0   \n",
       "49       NaN       NaN            2      3.586173              3.0   \n",
       "\n",
       "    conv_kernel_size  conv_stride  conv_pooling  dense_layers  dense_nodes  \n",
       "46                 5            2             3             1        450.0  \n",
       "4                  3            2             4             1        350.0  \n",
       "48                 3            2             3             1        500.0  \n",
       "19                 3            1             4             1        400.0  \n",
       "21                 5            2             3             1        450.0  \n",
       "1                  5            1             3             1        350.0  \n",
       "3                  5            2             4             1        300.0  \n",
       "20                 5            1             2             2        450.0  \n",
       "41                 3            1             2             2        450.0  \n",
       "11                 5            2             2             2        400.0  \n",
       "32                 3            1             4             1        350.0  \n",
       "2                  5            1             2             2        450.0  \n",
       "6                  5            2             2             2        450.0  \n",
       "9                  3            1             4             2        400.0  \n",
       "39                 5            2             2             2        350.0  \n",
       "35                 3            2             2             2        500.0  \n",
       "5                  5            1             2             2        350.0  \n",
       "33                 3            1             4             2        400.0  \n",
       "24                 3            1             2             2        350.0  \n",
       "14                 3            2             4             2        300.0  \n",
       "45                 3            2             2             2        350.0  \n",
       "0                  5            1             2             1        400.0  \n",
       "7                  3            2             2             1        500.0  \n",
       "8                  3            1             2             1        450.0  \n",
       "10                 3            2             2             1        300.0  \n",
       "12                 3            1             2             1        400.0  \n",
       "13                 3            1             4             2        300.0  \n",
       "15                 5            1             4             1        450.0  \n",
       "16                 3            2             2             2        450.0  \n",
       "17                 3            2             4             2        300.0  \n",
       "18                 3            2             4             1        350.0  \n",
       "22                 5            1             2             2        350.0  \n",
       "23                 5            1             3             1        400.0  \n",
       "25                 3            2             2             2        400.0  \n",
       "26                 3            1             4             1        500.0  \n",
       "27                 3            1             4             2        400.0  \n",
       "28                 5            1             3             2        450.0  \n",
       "29                 3            2             2             2        500.0  \n",
       "30                 3            2             3             2        350.0  \n",
       "31                 3            2             4             2        350.0  \n",
       "34                 3            2             2             1        450.0  \n",
       "36                 3            1             4             2        450.0  \n",
       "37                 5            1             4             2        350.0  \n",
       "38                 3            1             3             1        400.0  \n",
       "40                 3            1             2             2        350.0  \n",
       "42                 3            2             3             1        500.0  \n",
       "43                 5            2             2             1        350.0  \n",
       "44                 5            2             3             2        350.0  \n",
       "47                 5            2             3             1        500.0  \n",
       "49                 5            2             3             1        500.0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.sort_values('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [47:16<00:00, 56.72s/trial, best loss: 0.003879781099012456]   \n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "        \"inp_nodes\": inp.shape[1],\n",
    "        \"out_nodes\": out.shape[1], \n",
    "        \"batch_size\": 32,#[16, 32, 64, 128]\n",
    "        \"clip\": False, #hp.choice('clip',[True, False]),\n",
    "        \"final_activation\" : None,\n",
    "        \"optim\": 'adam',#hp.choice('optim',['adam', 'adagrad']),\n",
    "        \"lr\": 0.0001,#hp.choice('lr',[0.0001,0.001,.00001]),#hp.loguniform('lr', np.exp(np.log(1e-4)), np.exp(np.log(1e-1))),\n",
    "        \"batchnorm\": False,#hp.choice('batch_norm',[True, False]),\n",
    "        \"dropout_percent\": 0.0,#hp.choice('dropout',[0.0,0.1,0.2,0.3]),#hp.sample_from(lambda _: np.random.uniform(low=0.0, high=.6)),\n",
    "        \"shuffle\": True,\n",
    "        \"num_workers\": 4,\n",
    "        \"patience\": 5,\n",
    "        \"epochs\": 50,\n",
    "    \n",
    "        \"activation\": nn.LeakyReLU(),#nn.SELU(),\n",
    "        \"loss_f\": log_cosh_loss,#WSE,#hp.choice('loss_f',[WSE, nn.MSELoss()]), #, nn.L1Loss()\n",
    "    \n",
    "        \"conv_layers\": hp.choice('conv_layers', [1, 2]),\n",
    "        \"conv_filter_exp\": scope.int(hp.quniform('conv_filter_exp', 3, 4, q=1)),\n",
    "        \"conv_scaling\": hp.uniform('conv_scaling', 2, 4),\n",
    "        \"conv_kernel_size\": hp.choice('conv_kernel_size', [3, 5]),\n",
    "        \"conv_stride\": hp.choice('conv_stride', [1, 2]),\n",
    "        \"pooling\": hp.choice('pooling', [2, 3, 4]),\n",
    "        \"dense_layers\": hp.choice('dense_layers', [1, 2]),\n",
    "        \"dense_nodes\": scope.int(hp.quniform('dense_nodes', 300, 500, q=50)),\n",
    "        \"dense_scaling\": .5,#hp.uniform('dense_scaling', 0.25, 1),\n",
    "    }\n",
    "    \n",
    "btm = run_hopt(config, num_samples=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inp_nodes': 816,\n",
       " 'out_nodes': 384,\n",
       " 'batch_size': 32,\n",
       " 'clip': False,\n",
       " 'final_activation': None,\n",
       " 'optim': 'adam',\n",
       " 'lr': 0.0001,\n",
       " 'batchnorm': False,\n",
       " 'dropout_percent': 0.0,\n",
       " 'shuffle': True,\n",
       " 'num_workers': 4,\n",
       " 'patience': 5,\n",
       " 'epochs': 50,\n",
       " 'activation': LeakyReLU(negative_slope=0.01),\n",
       " 'loss_f': MSELoss(),\n",
       " 'conv_layers': 1.0,\n",
       " 'conv_filter_exp': 3.0,\n",
       " 'conv_scaling': 3.5607912740180527,\n",
       " 'conv_kernel_size': 5.0,\n",
       " 'conv_stride': 2.0,\n",
       " 'pooling': <hyperopt.pyll.base.Apply at 0x13d220290>,\n",
       " 'dense_layers': 1.0,\n",
       " 'dense_nodes': 450.0,\n",
       " 'dense_scaling': 0.5}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "conf_final=copy.deepcopy(config)\n",
    "\n",
    "for key,value in results_df.sort_values('loss').iloc[0].to_dict().items():\n",
    "    if key in conf_final:\n",
    "        conf_final[key]=value\n",
    "        \n",
    "conf_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_alone(config, model=Net, silent=True, checkpoint_dir=None):\n",
    "    \n",
    "    phases = ['train','val']\n",
    "\n",
    "    #x_train, x_test, y_train, y_test = data[0], data[1], data[2], data[3]\n",
    "\n",
    "    training_set = matchesDataset(x_train, y_train)\n",
    "    trainBatch = torch.utils.data.DataLoader(training_set, batch_size=config['batch_size'], shuffle=config['shuffle'], num_workers=config['num_workers'])\n",
    "\n",
    "    validation_set = matchesDataset(x_test, y_test)\n",
    "    valBatch = torch.utils.data.DataLoader(validation_set, batch_size=config['batch_size'], shuffle=config['shuffle'], num_workers=config['num_workers'])\n",
    "\n",
    "    earlStop = EarlyStopping(patience=int(config['patience']), keepBest=True)\n",
    "\n",
    "    net = model(int(config['num_layers']), int(config['num_nodes']), config['scaling_factor'], \n",
    "                int(config['num_nodes_out']), config['final_activation'], config['batch_norm'], config['dropout'])\n",
    "\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            net = nn.DataParallel(net)\n",
    "    net.to(device)\n",
    "\n",
    "    if checkpoint_dir:\n",
    "        model_state, optimizer_state = torch.load(\n",
    "            os.path.join(checkpoint_dir, \"checkpoint\"))\n",
    "        net.load_state_dict(model_state)\n",
    "        optimizer.load_state_dict(optimizer_state)\n",
    "\n",
    "    if config['optim']=='adam':\n",
    "        optimizer = Adam(net.parameters(), lr=config['lr'])\n",
    "    elif config['optim']=='adagrad':\n",
    "        optimizer = Adagrad(net.parameters(), lr=config['lr'])\n",
    "    else:\n",
    "        print('optim error')\n",
    "        return\n",
    "\n",
    "\n",
    "    losses=[[],[]]\n",
    "    mses=[]\n",
    "    diffs=[]\n",
    "    exit=False\n",
    "\n",
    "    for epoch in tqdm(range(config['epochs']), desc='Epoch'):\n",
    "    #for epoch in range(config['epochs']):\n",
    "\n",
    "        if exit:\n",
    "            break\n",
    "\n",
    "        for phase in phases:\n",
    "            if phase == 'train':\n",
    "                net.train(True) \n",
    "\n",
    "                \"\"\" Run the training of the model. \"\"\"    \n",
    "\n",
    "                losses_batch=[]\n",
    "                for batchNum, batch in enumerate(trainBatch):\n",
    "\n",
    "                    x = batch[0]\n",
    "                    y = batch[1]\n",
    "\n",
    "                    \"\"\" Move batches to GPU if available. \"\"\"\n",
    "\n",
    "                    if torch.cuda.is_available():\n",
    "                        x = x.cuda()\n",
    "                        y = y.cuda()\n",
    "\n",
    "                    \"\"\" Core of training. \"\"\"\n",
    "\n",
    "                    loss = config['loss_f'](net(x), y)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    if config['clip']:\n",
    "                        net.clp()\n",
    "\n",
    "                    losses_batch.append(loss)\n",
    "\n",
    "                \"\"\" Early stop check. \"\"\"\n",
    "\n",
    "                earlStop(loss, net)\n",
    "                finalepoch = epoch\n",
    "\n",
    "                if earlStop.earlyStop:\n",
    "\n",
    "                    if not silent:\n",
    "                        print('Limit loss improvement reached, stopping the training.')\n",
    "\n",
    "                    exit=True \n",
    "\n",
    "                #losses[0].append(torch.mean(torch.stack(losses_batch)).detach().cpu().numpy())\n",
    "            else:\n",
    "                net.train(False)\n",
    "                net.eval()\n",
    "\n",
    "                val_loss=0\n",
    "                val_mse=0\n",
    "\n",
    "                losses_batch=[]\n",
    "                for batchNum, batch in enumerate(valBatch):\n",
    "\n",
    "                    x = batch[0]\n",
    "                    y = batch[1]\n",
    "\n",
    "                    \"\"\" Move batches to GPU if available. \"\"\"\n",
    "\n",
    "                    if torch.cuda.is_available():\n",
    "                        x = x.cuda()\n",
    "                        y = y.cuda()\n",
    "\n",
    "                    \"\"\" Core of training. \"\"\"\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    output=net(x)\n",
    "                    target=y\n",
    "                    loss = config['loss_f'](output, target)\n",
    "\n",
    "                    #losses_batch.append(loss)\n",
    "                    val_loss+=loss.detach().numpy()\n",
    "                    val_mse+=nn.MSELoss()(output, target).detach().numpy()\n",
    "\n",
    "                #losses[1].append(torch.mean(torch.stack(losses_batch)).detach().cpu().numpy())\n",
    "\n",
    "                #with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
    "                #    path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "                #    torch.save((net.state_dict(), optimizer.state_dict()), path)\n",
    "\n",
    "                #tune.report(loss=(val_loss/batchNum), mse=(val_mse/batchNum))\n",
    "                #tune.report(loss=torch.mean(torch.stack(losses_batch)))\n",
    "\n",
    "    return net, val_loss/batchNum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  23%|██▎       | 23/100 [02:17<07:39,  5.97s/it]\n"
     ]
    }
   ],
   "source": [
    "net,loss=train_alone(conf_final, model=Net, silent=True, checkpoint_dir=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=net(torch.Tensor(inp)).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   goals  assists  cards_yellow  cards_red  own_goals  goals_against  saves\n",
      "0      2        1             2          0          0              1      3\n",
      "1      1        1             2          0          0              2      4\n",
      "   goals  assists  cards_yellow  cards_red  own_goals  goals_against  saves\n",
      "0      1        1             4          1          0              0      2\n",
      "1      0        0             1          0          0              1      1\n"
     ]
    }
   ],
   "source": [
    "i=1000\n",
    "cats=['minutes','goals','assists','cards_yellow','cards_red','own_goals']+['goals_against','saves']\n",
    "\n",
    "reframe, byteamframe = revert_output(pred[i])\n",
    "print(byteamframe.astype(int))\n",
    "reframe, byteamframe = revert_output(out[i])\n",
    "print(byteamframe.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RELU? He normal initialization, scaling elu?\n",
    "#Huber loss? MSLE?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
