{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import SGD, Adagrad, Adam, RMSprop, Adadelta\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from ray import tune\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "\n",
    "    \"\"\" Stops the training if loss doesn't improve after a given number of epochs. \"\"\"\n",
    "\n",
    "    def __init__(self, patience=3, epsilon=1e-5, keepBest=True, silent=True):\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): Number of epochs without change before stopping the learning (default 3).\n",
    "            epsilon (float): Minimum change in loss to be considered for early stopping (default 1e-5).\n",
    "            keepBest (bool): Keep track of the best model (memory consuming).\n",
    "        \"\"\"\n",
    "\n",
    "        self.patience = patience\n",
    "        self.epsilon = epsilon\n",
    "        self.counter = 0\n",
    "        \n",
    "        self.bestScore = np.inf\n",
    "     \n",
    "        self.keepBest = keepBest \n",
    "        self.bestModel = None\n",
    "\n",
    "        self.earlyStop = False\n",
    "        self.silent = silent\n",
    "\n",
    "    def __call__(self, loss, model):\n",
    "\n",
    "\n",
    "        \"\"\" Evaluate the loss change between epochs and activates early stop if below epsilon.\n",
    "\n",
    "        Args:\n",
    "            loss (float): current loss.\n",
    "            model (torch model): the current model.\n",
    "        \"\"\"\n",
    "\n",
    "        if loss > self.bestScore - self.epsilon:\n",
    "\n",
    "            self.counter += 1\n",
    "            if not self.silent:\n",
    "                print('EarlyStopping counter: {:d}/{:d}'.format(self.counter,self.patience))\n",
    "\n",
    "            if self.counter >= self.patience:\n",
    "                self.earlyStop = True\n",
    "\n",
    "        else:   \n",
    "\n",
    "            self.counter = 0\n",
    "            self.bestScore = loss\n",
    "\n",
    "            if self.keepBest:\n",
    "                self.bestModel = copy.deepcopy(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class matchesDataset(Dataset):\n",
    "\n",
    "    \"\"\" Extend pytorch Dataset class to include cleaning and training set creation, \"\"\"\n",
    "    \n",
    "    def __init__(self, matches, results):\n",
    "\n",
    "        self.matches = torch.tensor(matches, dtype=torch.float32)\n",
    "        self.results = torch.tensor(results, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        \"\"\" Returns the len of the training sample. \"\"\"\n",
    "        \n",
    "        return len(self.matches)\n",
    "        \n",
    "\n",
    "    def __getitem__(self, index): \n",
    "\n",
    "        \"\"\" Returns a word, a context word and a list of negative words for training for a given index. \n",
    "\n",
    "        Args:\n",
    "            index (int): index for the word selection.\n",
    "\n",
    "        Returns:\n",
    "            (string, string, list of strings): selected word, context word and a randomly drawn list \n",
    "                                               of negative words.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.matches[index], self.results[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogCoshLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, y_t, y_prime_t):\n",
    "        ey_t = y_t - y_prime_t\n",
    "        return torch.mean(torch.log(torch.cosh(ey_t + 1e-12)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/quirky-keras-custom-and-asymmetric-loss-functions-for-keras-in-r-a8b5271171fe\n",
    "def WSE(output, target, a=1.5, b=.5):\n",
    "    loss = torch.mean(a*torch.minimum(torch.zeros(output.shape[1]),output - target)**2+\\\n",
    "                      b*torch.maximum(torch.zeros(output.shape[1]),output - target)**2)      \n",
    "    return loss\n",
    "\n",
    "def WSEl1(output, target, a=1.5, b=.5):\n",
    "    loss = torch.mean(a*torch.abs(torch.minimum(torch.zeros(output.shape[1]),output - target))+\\\n",
    "                      b*torch.abs(torch.maximum(torch.zeros(output.shape[1]),output - target)))      \n",
    "    return loss\n",
    "\n",
    "def WSE2(output, target, a=1.5, b=.5):\n",
    "    loss = np.mean(a*np.minimum(np.zeros(output.shape[0]),output - target)**2+\\\n",
    "                      b*np.maximum(np.zeros(output.shape[0]),output - target)**2)      \n",
    "    return loss\n",
    "\n",
    "def WSEl12(output, target, a=1.5, b=.5):\n",
    "    loss = np.mean(a*np.abs(np.minimum(np.zeros(output.shape[0]),output - target))+\\\n",
    "                      b*np.abs(np.maximum(np.zeros(output.shape[0]),output - target)))      \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_layers, num_nodes, scaling_factor, num_nodes_out, final_activation, dropout_percent, batchnorm):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.fc = []\n",
    "        self.lr = []\n",
    "        self.fact = final_activation\n",
    "        self.nl = num_layers\n",
    "        self.bn = []\n",
    "        self.dp = []\n",
    "        power=0\n",
    "        \n",
    "        for i in np.arange(self.nl, dtype = int):\n",
    "            self.fc.append(nn.Linear(int(num_nodes*(scaling_factor**power)), int(num_nodes*(scaling_factor**(power+1)))))\n",
    "            self.lr.append(nn.LeakyReLU())\n",
    "            if batchnorm:\n",
    "                self.bn.append(nn.BatchNorm1d(int(num_nodes*(scaling_factor**(power+1)))))\n",
    "            if dropout_percent>0:\n",
    "                self.dp.append(nn.Dropout(dropout_percent))\n",
    "            power+=1\n",
    "            \n",
    "        self.oupt = nn.Linear(int(num_nodes*(scaling_factor**power)), int(num_nodes_out))\n",
    "     \n",
    "    def reset_weights(self):\n",
    "\n",
    "        \"\"\" Resets network weights according to chosen distribution. \"\"\"\n",
    "\n",
    "        for f in self.fc:\n",
    "            nn.init.xavier_uniform_(f.weight, gain=1)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        z = x\n",
    "        for i in range(self.nl):\n",
    "            z = self.fc[i](z)\n",
    "            if len(self.bn)>0:\n",
    "                z=self.bn[i](z)\n",
    "            z=self.lr[i](z)\n",
    "            if len(self.dp)>0:\n",
    "                z=self.dp[i](z)\n",
    "                \n",
    "        if self.fact is not None:\n",
    "            z = self.oupt(self.fact(z))\n",
    "        else:\n",
    "            z = self.oupt(z)\n",
    "        return z\n",
    "    \n",
    "    def clp(self):\n",
    "        with torch.no_grad():\n",
    "            for i in range(self.nl):\n",
    "                self.fc[i].weight.copy_ (self.fc[i].weight.data.clamp(min=0)) \n",
    "            self.oupt.weight.copy_ (self.oupt.weight.data.clamp(min=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_layers, num_nodes, embed_dim, num_nodes_out, final_activation, dropout_percent, batchnorm):\n",
    "        super(AE, self).__init__()\n",
    "        \n",
    "        self.fc = []\n",
    "        self.lr = []\n",
    "        self.fact = final_activation\n",
    "        self.nl = num_layers\n",
    "        self.bn = []\n",
    "        self.dp = []\n",
    "                \n",
    "        for i in np.arange(self.nl, dtype = int):\n",
    "            self.fc.append(nn.Linear(int(num_nodes+i*1.0*(embed_dim-num_nodes)/(self.nl)), int(num_nodes+(i+1)*1.0*(embed_dim-num_nodes)/(self.nl))))\n",
    "            print(int(num_nodes+i*1.0*(embed_dim-num_nodes)/(self.nl+1)), int(num_nodes+(i+1)*1.0*(embed_dim-num_nodes)/(self.nl)))\n",
    "            self.lr.append(nn.LeakyReLU())\n",
    "            if batchnorm:\n",
    "                self.bn.append(nn.BatchNorm1d(int(num_nodes+(i+1)*1.0*(embed_dim-num_nodes)/(self.nl))))\n",
    "            if dropout_percent>0:\n",
    "                self.dp.append(nn.Dropout(dropout_percent))\n",
    "                \n",
    "        print()\n",
    "        \n",
    "        for i in np.arange(self.nl, dtype = int)[:-1][::-1]:\n",
    "            self.fc.append(nn.Linear(int(num_nodes_out+(i+2)*1.0*(embed_dim-num_nodes_out)/self.nl), int(num_nodes_out+(i+1)*1.0*(embed_dim-num_nodes_out)/self.nl)))\n",
    "            print(int(num_nodes_out+(i+2)*1.0*(embed_dim-num_nodes_out)/self.nl), int(num_nodes_out+(i+1)*1.0*(embed_dim-num_nodes_out)/self.nl))\n",
    "            self.lr.append(nn.LeakyReLU())\n",
    "            if batchnorm:\n",
    "                self.bn.append(nn.BatchNorm1d(int(num_nodes_out+(i+1)*1.0*(embed_dim-num_nodes_out)/self.nl)))\n",
    "            if dropout_percent>0:\n",
    "                self.dp.append(nn.Dropout(dropout_percent))\n",
    "         \n",
    "        self.oupt = nn.Linear(int(num_nodes_out+1.0*(embed_dim-num_nodes_out)/self.nl), int(num_nodes_out))\n",
    "        print(int(num_nodes_out+1.0*(embed_dim-num_nodes_out)/self.nl), int(num_nodes_out))\n",
    "        \n",
    "    def reset_weights(self):\n",
    "\n",
    "        \"\"\" Resets network weights according to chosen distribution. \"\"\"\n",
    "\n",
    "        for f in self.fc:\n",
    "            nn.init.xavier_uniform_(f.weight, gain=1)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        z = x\n",
    "        for i in range(len(self.fc)):\n",
    "            z = self.fc[i](z)\n",
    "            if len(self.bn)>0:\n",
    "                z=self.bn[i](z)\n",
    "            z=self.lr[i](z)\n",
    "            if len(self.dp)>0:\n",
    "                z=self.dp[i](z)\n",
    "                \n",
    "        if self.fact is not None:\n",
    "            z = self.oupt(self.fact(z))\n",
    "        else:\n",
    "            z = self.oupt(z)\n",
    "        return z\n",
    "    \n",
    "    def clp(self):\n",
    "        with torch.no_grad():\n",
    "            for i in range(self.nl):\n",
    "                self.fc[i].weight.copy_ (self.fc[i].weight.data.clamp(min=0)) \n",
    "            self.oupt.weight.copy_ (self.oupt.weight.data.clamp(min=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import floor\n",
    "\n",
    "def conv_out_shape(h_w, kernel_size=1, stride=1, pad=0, dilation=1):\n",
    "\n",
    "\tif isinstance(h_w, list):\n",
    "\t    if type(kernel_size) is not tuple:\n",
    "\t        kernel_size = (kernel_size, kernel_size)\n",
    "\t    h = floor( ((h_w[0] + (2 * pad) - ( dilation * (kernel_size[0] - 1) ) - 1 )/ stride) + 1)\n",
    "\t    w = floor( ((h_w[1] + (2 * pad) - ( dilation * (kernel_size[1] - 1) ) - 1 )/ stride) + 1)\n",
    "\t    return h, w\n",
    "\telse:\n",
    "\t\treturn floor( ((h_w + (2 * pad) - ( dilation * (kernel_size - 1) ) - 1 )/ stride) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, inp_nodes, conv_layers, conv_nodes, conv_scaling, pooling, conv_kernels, conv_stride, \n",
    "                 dense_layers, dense_nodes, dense_scaling, out_nodes, final_activation, dropout_percent, batchnorm):\n",
    "        super(CNet, self).__init__()\n",
    "        \n",
    "        self.fc = []\n",
    "        self.cv = []\n",
    "        self.lr_cv = []\n",
    "        self.lr_fc = []\n",
    "        self.pl = []\n",
    "        self.fact = final_activation\n",
    "        self.cl = conv_layers\n",
    "        self.dl = dense_layers\n",
    "        self.bn_fc = []\n",
    "        self.bn_cv = []\n",
    "        self.dp = []\n",
    "        \n",
    "        if pooling<=0:\n",
    "            pooling=1\n",
    "            \n",
    "        power=0\n",
    "        for i in range(self.cl):\n",
    "            if i==0:\n",
    "                self.cv.append(nn.Conv1d(in_channels = 1,\n",
    "                                    out_channels = conv_nodes,\n",
    "                                    kernel_size = conv_kernels,\n",
    "                                    stride = conv_stride))\n",
    "                cos=int(conv_out_shape(inp_nodes,\n",
    "                                    kernel_size = conv_kernels,\n",
    "                                    stride = conv_stride)/pooling)\n",
    "            else:\n",
    "                self.cv.append(nn.Conv1d(in_channels =  int(conv_nodes*(conv_scaling**(power-1))),\n",
    "                                    out_channels = int(conv_nodes*(conv_scaling**(power-1))),\n",
    "                                    kernel_size = conv_kernels,\n",
    "                                    stride = conv_stride))\n",
    "                cos=int(conv_out_shape(cos,\n",
    "                                    kernel_size = conv_kernels,\n",
    "                                    stride = conv_stride)/pooling)\n",
    "                \n",
    "            self.lr_cv.append(nn.LeakyReLU())\n",
    "            #model.append(nn.Tanh())\n",
    "\n",
    "            #scqa\n",
    "            if pooling>1:\n",
    "                self.pl.append(nn.MaxPool1d(pooling))\n",
    "            if batchnorm:\n",
    "                self.bn_cv.append(nn.BatchNorm1d(conv_nodes))\n",
    "                                                            \n",
    "            power+=1\n",
    "            \n",
    "        self.flat = nn.Flatten()\n",
    "                \n",
    "        power=0\n",
    "        for j in range(self.dl): \n",
    "            if j==0:\n",
    "                self.fc.append(nn.Linear(in_features = int(cos*conv_nodes),\n",
    "                                    out_features = dense_nodes))\n",
    "            else:\n",
    "                self.fc.append(nn.Linear(in_features = int(dense_nodes*(dense_scaling**(power-1))),\n",
    "                                    out_features = int(dense_nodes*(dense_scaling**power))))\n",
    "     \n",
    "            self.lr_fc.append(nn.LeakyReLU())\n",
    "                                                           \n",
    "            if batchnorm:\n",
    "                self.bn_fc.append(nn.BatchNorm1d(int(dense_nodes*(dense_scaling**power))))\n",
    "            if dropout_percent>0:\n",
    "                self.dp.append(nn.Dropout(dropout_percent))\n",
    "            power+=1\n",
    "            \n",
    "        self.oupt = nn.Linear(int(dense_nodes*(dense_scaling**(power-1))), int(out_nodes))\n",
    "     \n",
    "    def reset_weights(self):\n",
    "\n",
    "        \"\"\" Resets network weights according to chosen distribution. \"\"\"\n",
    "\n",
    "        for f in self.cv:\n",
    "            nn.init.xavier_uniform_(f.weight, gain=1)\n",
    "        for f in self.fc:\n",
    "            nn.init.xavier_uniform_(f.weight, gain=1)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        z = x\n",
    "        for i in range(self.cl):        \n",
    "            z = self.cv[i](z)\n",
    "            if len(self.pl)>0:\n",
    "                z=self.pl[i](z)\n",
    "            if len(self.bn_cv)>0:\n",
    "                z=self.bn_cv[i](z)\n",
    "            z=self.lr_cv[i](z)     \n",
    "                        \n",
    "        z = self.flat(z)\n",
    "                                                           \n",
    "        for i in range(self.dl):\n",
    "            z = self.fc[i](z)\n",
    "            if len(self.bn_fc)>0:\n",
    "                z=self.bn_fc[i](z)\n",
    "            z=self.lr_fc[i](z)\n",
    "            if len(self.dp)>0:\n",
    "                z=self.dp[i](z)\n",
    "                \n",
    "        if self.fact is not None:\n",
    "            z = self.oupt(self.fact(z))\n",
    "        else:\n",
    "            z = self.oupt(z)\n",
    "        return z\n",
    "    \n",
    "    def clp(self):\n",
    "        with torch.no_grad():\n",
    "            for i in range(self.nl):\n",
    "                self.fc[i].weight.copy_ (self.fc[i].weight.data.clamp(min=0)) \n",
    "            self.oupt.weight.copy_ (self.oupt.weight.data.clamp(min=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config, model=Net, silent=True, checkpoint_dir=None):\n",
    "    \n",
    "    phases = ['train','val']\n",
    "    \n",
    "    #x_train, x_test, y_train, y_test = data[0], data[1], data[2], data[3]\n",
    "    \n",
    "    training_set = matchesDataset(x_train, y_train)\n",
    "    trainBatch = torch.utils.data.DataLoader(training_set, batch_size=config['batch_size'], shuffle=config['shuffle'], num_workers=config['num_workers'])\n",
    "\n",
    "    validation_set = matchesDataset(x_test, y_test)\n",
    "    valBatch = torch.utils.data.DataLoader(validation_set, batch_size=config['batch_size'], shuffle=config['shuffle'], num_workers=config['num_workers'])\n",
    "\n",
    "    earlStop = EarlyStopping(patience=config['patience'], keepBest=False)\n",
    "    \n",
    "    if 'scaling_factor' in config:\n",
    "        net = model(config['num_layers'], config['num_nodes'], config['scaling_factor'], \n",
    "                    config['num_nodes_out'], config['final_activation'])\n",
    "    elif 'embed_dim' in config:\n",
    "        net = model(config['num_layers'], config['num_nodes'], config['embed_dim'], \n",
    "                    config['num_nodes_out'], config['final_activation'])\n",
    "        \n",
    "        \n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            net = nn.DataParallel(net)\n",
    "    net.to(device)\n",
    "    \n",
    "    if checkpoint_dir:\n",
    "        model_state, optimizer_state = torch.load(\n",
    "            os.path.join(checkpoint_dir, \"checkpoint\"))\n",
    "        net.load_state_dict(model_state)\n",
    "        optimizer.load_state_dict(optimizer_state)\n",
    "\n",
    "    if config['optim']=='adam':\n",
    "        optimizer = Adam(net.parameters(), lr=config['lr'])\n",
    "    elif config['optim']=='adagrad':\n",
    "        optimizer = Adagrad(net.parameters(), lr=config['lr'])\n",
    "    elif config['optim']=='adadelta':\n",
    "        optimizer = Adadelta(net.parameters(), lr=config['lr'])\n",
    "    elif config['optim']=='rmsprop':\n",
    "        optimizer = RMSprop(net.parameters(), lr=config['lr']) \n",
    "    else:\n",
    "        print('optim error')\n",
    "        return\n",
    "    \n",
    "    \n",
    "    losses=[[],[]]\n",
    "    mses=[]\n",
    "    diffs=[]\n",
    "    exit=False\n",
    "    \n",
    "    #for epoch in tqdm(range(epochs), desc='Epoch'):\n",
    "    for epoch in range(config['epochs']):\n",
    "\n",
    "        if exit:\n",
    "            break\n",
    "            \n",
    "        for phase in phases:\n",
    "            if phase == 'train':\n",
    "                net.train(True) \n",
    "\n",
    "                \"\"\" Run the training of the model. \"\"\"    \n",
    "\n",
    "                losses_batch=[]\n",
    "                for batchNum, batch in enumerate(trainBatch):\n",
    "\n",
    "                    x = batch[0]\n",
    "                    y = batch[1]\n",
    "\n",
    "                    \"\"\" Move batches to GPU if available. \"\"\"\n",
    "\n",
    "                    if torch.cuda.is_available():\n",
    "                        x = x.cuda()\n",
    "                        y = y.cuda()\n",
    "\n",
    "                    \"\"\" Core of training. \"\"\"\n",
    "                    \n",
    "                    loss = config['loss_f'](net(x), y)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    if config['clip']:\n",
    "                        net.clp()\n",
    "\n",
    "                    losses_batch.append(loss)\n",
    "\n",
    "                \"\"\" Early stop check. \"\"\"\n",
    "\n",
    "                earlStop(loss, net)\n",
    "                finalepoch = epoch\n",
    "                \n",
    "                if earlStop.earlyStop:\n",
    "\n",
    "                    if not silent:\n",
    "                        print('Limit loss improvement reached, stopping the training.')\n",
    "                        \n",
    "                    exit=True \n",
    "                \n",
    "                #losses[0].append(torch.mean(torch.stack(losses_batch)).detach().cpu().numpy())\n",
    "            else:\n",
    "                net.train(False)\n",
    "                net.eval()\n",
    "                \n",
    "                val_loss=0\n",
    "                val_mse=0\n",
    "      \n",
    "                losses_batch=[]\n",
    "                for batchNum, batch in enumerate(valBatch):\n",
    "\n",
    "                    x = batch[0]\n",
    "                    y = batch[1]\n",
    "\n",
    "                    \"\"\" Move batches to GPU if available. \"\"\"\n",
    "\n",
    "                    if torch.cuda.is_available():\n",
    "                        x = x.cuda()\n",
    "                        y = y.cuda()\n",
    "\n",
    "                    \"\"\" Core of training. \"\"\"\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    output=net(x)\n",
    "                    target=y\n",
    "                    loss = config['loss_f'](output, target)\n",
    "\n",
    "                    #losses_batch.append(loss)\n",
    "                    val_loss+=loss.detach().numpy()\n",
    "                    val_mse+=nn.MSELoss()(output, target).detach().numpy()\n",
    "                    \n",
    "                #losses[1].append(torch.mean(torch.stack(losses_batch)).detach().cpu().numpy())\n",
    "                \n",
    "                with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
    "                    path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "                    torch.save((net.state_dict(), optimizer.state_dict()), path)\n",
    "\n",
    "                tune.report(loss=(val_loss/batchNum), mse=(val_mse/batchNum))\n",
    "                #tune.report(loss=torch.mean(torch.stack(losses_batch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revert_output(output,lineup=None,dropdummy=False):\n",
    "\n",
    "    reframe=pd.DataFrame(copy.deepcopy(output.reshape(48,8)),\n",
    "                 columns=['minutes','goals','assists','cards_yellow','cards_red','own_goals','goals_against','saves'])\n",
    "    \n",
    "    reframe[reframe<0] = 0\n",
    "    reframe['team'] = 0\n",
    "    reframe['team'].iloc[25:]=1\n",
    "    reframe['minutes']*=90\n",
    "    \n",
    "    if lineup is not None:\n",
    "        reframe.index=lineup\n",
    "        if dropdummy:\n",
    "            reframe.drop([x for x in reframe.index if x.startswith('dummy')], axis=0, inplace=True)\n",
    "  \n",
    "    byteamframe=pd.concat([reframe[reframe['team']==0].sum(axis=0),reframe[reframe['team']==1].sum(axis=0)], axis=1).T\n",
    "    \n",
    "    return reframe, byteamframe[byteamframe.columns[1:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
