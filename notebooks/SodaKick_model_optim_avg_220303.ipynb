{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import sys, getopt\n",
    "import csv\n",
    "import pickle\n",
    "import copy\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "%matplotlib inline  \n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "import umap\n",
    "from sklearn.decomposition import TruncatedSVD as tsvd\n",
    "\n",
    "def nearZeroVarDropAuto(df,thresh=0.99):\n",
    "    vVal=df.var(axis=0).values\n",
    "    cs=pd.Series(vVal).sort_values(ascending=False).cumsum()\n",
    "    remove=cs[cs>cs.values[-1]*thresh].index.values\n",
    "    return df.drop(df.columns[remove],axis=1)\n",
    "\n",
    "%run SodaKick_download_functions.ipynb\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import SGD, Adagrad, Adam, Adagrad\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from ray import tune\n",
    "#from ray.tune import CLIReporter\n",
    "#from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "from hyperopt import hp, tpe, fmin, Trials\n",
    "from hyperopt import STATUS_OK, STATUS_FAIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "\n",
    "    \"\"\" Stops the training if loss doesn't improve after a given number of epochs. \"\"\"\n",
    "\n",
    "    def __init__(self, patience=3, epsilon=1e-5, keepBest=True, silent=True):\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): Number of epochs without change before stopping the learning (default 3).\n",
    "            epsilon (float): Minimum change in loss to be considered for early stopping (default 1e-5).\n",
    "            keepBest (bool): Keep track of the best model (memory consuming).\n",
    "        \"\"\"\n",
    "\n",
    "        self.patience = patience\n",
    "        self.epsilon = epsilon\n",
    "        self.counter = 0\n",
    "        \n",
    "        self.bestScore = np.inf\n",
    "     \n",
    "        self.keepBest = keepBest \n",
    "        self.bestModel = None\n",
    "\n",
    "        self.earlyStop = False\n",
    "        self.silent = silent\n",
    "\n",
    "    def __call__(self, loss, model):\n",
    "\n",
    "\n",
    "        \"\"\" Evaluate the loss change between epochs and activates early stop if below epsilon.\n",
    "\n",
    "        Args:\n",
    "            loss (float): current loss.\n",
    "            model (torch model): the current model.\n",
    "        \"\"\"\n",
    "\n",
    "        if loss > self.bestScore - self.epsilon:\n",
    "\n",
    "            self.counter += 1\n",
    "            if not self.silent:\n",
    "                print('EarlyStopping counter: {:d}/{:d}'.format(self.counter,self.patience))\n",
    "\n",
    "            if self.counter >= self.patience:\n",
    "                self.earlyStop = True\n",
    "\n",
    "        else:   \n",
    "\n",
    "            self.counter = 0\n",
    "            self.bestScore = loss\n",
    "\n",
    "            if self.keepBest:\n",
    "                self.bestModel = copy.deepcopy(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class matchesDataset(Dataset):\n",
    "\n",
    "    \"\"\" Extend pytorch Dataset class to include cleaning and training set creation, \"\"\"\n",
    "    \n",
    "    def __init__(self, matches, results):\n",
    "\n",
    "        self.matches = torch.tensor(matches, dtype=torch.float32)\n",
    "        self.results = torch.tensor(results, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        \"\"\" Returns the len of the training sample. \"\"\"\n",
    "        \n",
    "        return len(self.matches)\n",
    "        \n",
    "\n",
    "    def __getitem__(self, index): \n",
    "\n",
    "        \"\"\" Returns a word, a context word and a list of negative words for training for a given index. \n",
    "\n",
    "        Args:\n",
    "            index (int): index for the word selection.\n",
    "\n",
    "        Returns:\n",
    "            (string, string, list of strings): selected word, context word and a randomly drawn list \n",
    "                                               of negative words.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.matches[index], self.results[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/quirky-keras-custom-and-asymmetric-loss-functions-for-keras-in-r-a8b5271171fe\n",
    "#weighted asimmetric square error, errors by going below the value (not seeing a goal when it's there) are weighted more\n",
    "\n",
    "def WSE(output, target, a=1.5, b=.5):\n",
    "    loss = torch.mean(a/(a+b)*torch.minimum(torch.zeros(output.shape[1]),output - target)**2+\\\n",
    "                      b/(a+b)*torch.maximum(torch.zeros(output.shape[1]),output - target)**2)      \n",
    "    return loss\n",
    "\n",
    "def WSEl1(output, target, a=1.5, b=.5):\n",
    "    loss = torch.mean(a/(a+b)*torch.abs(torch.minimum(torch.zeros(output.shape[1]),output - target))+\\\n",
    "                      b/(a+b)*torch.abs(torch.maximum(torch.zeros(output.shape[1]),output - target)))      \n",
    "    return loss\n",
    "\n",
    "def WSE2(output, target, a=1.5, b=.5):\n",
    "    loss = np.mean(a/(a+b)*np.minimum(np.zeros(output.shape[0]),output - target)**2+\\\n",
    "                      b/(a+b)*np.maximum(np.zeros(output.shape[0]),output - target)**2)      \n",
    "    return loss\n",
    "\n",
    "def WSEl12(output, target, a=1.5, b=.5):\n",
    "    loss = np.mean(a/(a+b)*np.abs(np.minimum(np.zeros(output.shape[0]),output - target))+\\\n",
    "                      b/(a+b)*np.abs(np.maximum(np.zeros(output.shape[0]),output - target)))      \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_mins(vec):\n",
    "    for i in range(vec.shape[0]):\n",
    "        vec[i][::8]=vec[i][::8]/90\n",
    "\n",
    "def NormalizeData(data):\n",
    "    return (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "\n",
    "def NormalizeMatrix(data):   \n",
    "    for i in range(data.shape[1]):\n",
    "        data[:,i] = NormalizeData(data[:,i])\n",
    "\n",
    "def norm_max(out):\n",
    "    \n",
    "    maxes=[]\n",
    "    for i in range(int(out.shape[1]/8.0)):\n",
    "        maxes.append(out[:,8*int(i):8*(int(i)+1)].max(axis=0))\n",
    "\n",
    "        #maxes.append(out.max(axis=1)[8*int(i):8*(int(i)+1):8])\n",
    "    denominator=np.tile(np.max(maxes,axis=0),int(out.shape[1]/8))\n",
    "    return out/denominator, denominator \n",
    "\n",
    "with open(r'/Users/federico comitani/GitHub/sodakick/data/ainp_220303.pkl', 'rb') as pk:\n",
    "    inp=pickle.load(pk)\n",
    "with open(r'/Users/federico comitani/GitHub/sodakick/data/out_220303.pkl', 'rb') as pk:\n",
    "    out=np.array(pickle.load(pk),dtype=float)\n",
    "    \n",
    "### skipping norm for now since it's already tsvd \n",
    "#NormalizeMatrix(inp)\n",
    "#np.nan_to_num(inp, copy=False)\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "inp = scaler.fit_transform(inp)\n",
    "\n",
    "#normalize_mins(out)\n",
    "out, denominator= norm_max(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "         inp[:25000], out[:25000], test_size=0.2, random_state=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_layers, num_nodes, scaling_factor, num_nodes_out, final_activation, batch_norm, dropout):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.fc = []\n",
    "        self.lr = []\n",
    "        self.bn = []\n",
    "        self.dp = []\n",
    "        self.fact = final_activation\n",
    "        self.nl = num_layers\n",
    "        self.batch_norm = batch_norm\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        power=0\n",
    "        \n",
    "        for i in range(self.nl):\n",
    "            self.fc.append(nn.Linear(int(num_nodes*(scaling_factor**power)), int(num_nodes*(scaling_factor**(power+1)))))\n",
    "            self.lr.append(nn.LeakyReLU())\n",
    "            \n",
    "            if self.batch_norm:\n",
    "                self.bn.append(nn.BatchNorm1d(int(num_nodes*(scaling_factor**(power+1)))))\n",
    "\n",
    "            if self.dropout>0.0:\n",
    "                self.dp.append(nn.Dropout(dropout))\n",
    "                \n",
    "            power+=1\n",
    "        \n",
    "        self.oupt = nn.Linear(int(num_nodes*(scaling_factor**power)), int(num_nodes_out))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = x\n",
    "        for i in range(self.nl):\n",
    "            \n",
    "            z = self.fc[i](z)\n",
    "            \n",
    "            if self.batch_norm:\n",
    "                z = self.bn[i](z)\n",
    "            \n",
    "            z = self.lr[i](z)\n",
    "        \n",
    "            if self.dropout>0.0:\n",
    "                z = self.dp[i](z)\n",
    "                \n",
    "        if self.fact is not None:\n",
    "            z = self.fact(z)\n",
    "        \n",
    "        z = self.oupt(z)\n",
    "        \n",
    "        return z\n",
    "    \n",
    "    def clp(self):\n",
    "        with torch.no_grad():\n",
    "            for i in range(self.nl):\n",
    "                self.fc[i].weight.copy_ (self.fc[i].weight.data.clamp(min=0)) \n",
    "            self.oupt.weight.copy_ (self.oupt.weight.data.clamp(min=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config, model=Net, silent=True, checkpoint_dir=None):\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        phases = ['train','val']\n",
    "\n",
    "        #x_train, x_test, y_train, y_test = data[0], data[1], data[2], data[3]\n",
    "\n",
    "        training_set = matchesDataset(x_train, y_train)\n",
    "        trainBatch = torch.utils.data.DataLoader(training_set, batch_size=config['batch_size'], shuffle=config['shuffle'], num_workers=config['num_workers'])\n",
    "\n",
    "        validation_set = matchesDataset(x_test, y_test)\n",
    "        valBatch = torch.utils.data.DataLoader(validation_set, batch_size=config['batch_size'], shuffle=config['shuffle'], num_workers=config['num_workers'])\n",
    "\n",
    "        earlStop = EarlyStopping(patience=config['patience'], keepBest=False)\n",
    "\n",
    "        net = model(config['num_layers'], config['num_nodes'], config['scaling_factor'], \n",
    "                    config['num_nodes_out'], config['final_activation'], config['batch_norm'], config['dropout'])\n",
    "\n",
    "        device = \"cpu\"\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda:0\"\n",
    "            if torch.cuda.device_count() > 1:\n",
    "                net = nn.DataParallel(net)\n",
    "        net.to(device)\n",
    "\n",
    "        if checkpoint_dir:\n",
    "            model_state, optimizer_state = torch.load(\n",
    "                os.path.join(checkpoint_dir, \"checkpoint\"))\n",
    "            net.load_state_dict(model_state)\n",
    "            optimizer.load_state_dict(optimizer_state)\n",
    "\n",
    "        if config['optim']=='adam':\n",
    "            optimizer = Adam(net.parameters(), lr=config['lr'])\n",
    "        elif config['optim']=='adagrad':\n",
    "            optimizer = Adagrad(net.parameters(), lr=config['lr'])\n",
    "        else:\n",
    "            print('optim error')\n",
    "            return\n",
    "\n",
    "\n",
    "        losses=[[],[]]\n",
    "        mses=[]\n",
    "        diffs=[]\n",
    "        exit=False\n",
    "\n",
    "        #for epoch in tqdm(range(epochs), desc='Epoch'):\n",
    "        for epoch in range(config['epochs']):\n",
    "\n",
    "            if exit:\n",
    "                break\n",
    "\n",
    "            for phase in phases:\n",
    "                if phase == 'train':\n",
    "                    net.train(True) \n",
    "\n",
    "                    \"\"\" Run the training of the model. \"\"\"    \n",
    "\n",
    "                    losses_batch=[]\n",
    "                    for batchNum, batch in enumerate(trainBatch):\n",
    "\n",
    "                        x = batch[0]\n",
    "                        y = batch[1]\n",
    "\n",
    "                        \"\"\" Move batches to GPU if available. \"\"\"\n",
    "\n",
    "                        if torch.cuda.is_available():\n",
    "                            x = x.cuda()\n",
    "                            y = y.cuda()\n",
    "\n",
    "                        \"\"\" Core of training. \"\"\"\n",
    "\n",
    "                        loss = config['loss_f'](net(x), y)\n",
    "\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        if config['clip']:\n",
    "                            net.clp()\n",
    "\n",
    "                        losses_batch.append(loss)\n",
    "\n",
    "                    \"\"\" Early stop check. \"\"\"\n",
    "\n",
    "                    earlStop(loss, net)\n",
    "                    finalepoch = epoch\n",
    "\n",
    "                    if earlStop.earlyStop:\n",
    "\n",
    "                        if not silent:\n",
    "                            print('Limit loss improvement reached, stopping the training.')\n",
    "\n",
    "                        exit=True \n",
    "\n",
    "                    #losses[0].append(torch.mean(torch.stack(losses_batch)).detach().cpu().numpy())\n",
    "                else:\n",
    "                    net.train(False)\n",
    "                    net.eval()\n",
    "\n",
    "                    val_loss=0\n",
    "                    val_mse=0\n",
    "\n",
    "                    losses_batch=[]\n",
    "                    for batchNum, batch in enumerate(valBatch):\n",
    "\n",
    "                        x = batch[0]\n",
    "                        y = batch[1]\n",
    "\n",
    "                        \"\"\" Move batches to GPU if available. \"\"\"\n",
    "\n",
    "                        if torch.cuda.is_available():\n",
    "                            x = x.cuda()\n",
    "                            y = y.cuda()\n",
    "\n",
    "                        \"\"\" Core of training. \"\"\"\n",
    "\n",
    "                        optimizer.zero_grad()\n",
    "                        output=net(x)\n",
    "                        target=y\n",
    "                        loss = config['loss_f'](output, target)\n",
    "\n",
    "                        #losses_batch.append(loss)\n",
    "                        val_loss+=loss.detach().numpy()\n",
    "                        val_mse+=nn.MSELoss()(output, target).detach().numpy()\n",
    "\n",
    "                    #losses[1].append(torch.mean(torch.stack(losses_batch)).detach().cpu().numpy())\n",
    "\n",
    "                    #with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
    "                    #    path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "                    #    torch.save((net.state_dict(), optimizer.state_dict()), path)\n",
    "\n",
    "                    #tune.report(loss=(val_loss/batchNum), mse=(val_mse/batchNum))\n",
    "                    #tune.report(loss=torch.mean(torch.stack(losses_batch)))\n",
    "\n",
    "        return {'loss': (val_loss/batchNum), 'status': STATUS_OK , 'mse': (val_mse/batchNum)}\n",
    "    \n",
    "    except:\n",
    "        \n",
    "        return {'loss': np.nan, 'status': STATUS_FAIL, 'mse': np.nan}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revert_output(output,multiplier=denominator,lineup=None):\n",
    "\n",
    "    reframe=pd.DataFrame(output.reshape(48,8),\n",
    "                 columns=['minutes','goals','assists','cards_yellow','cards_red','own_goals','goals_against','saves'])\n",
    "    \n",
    "    reframe[reframe<0] = 0\n",
    "    if lineup is not None:\n",
    "        reframe.index=lineup\n",
    "        reframe.drop([x for x in reframe.index if x.startswith('dummy')], axis=0, inplace=True)\n",
    "    \n",
    "    \n",
    "    #reframe['minutes']*=90\n",
    "    reframe=reframe*denominator[:8]\n",
    "    byteamframe=pd.concat([reframe.iloc[:24,:].sum(axis=0),reframe.iloc[24:,:].sum(axis=0)], axis=1).T\n",
    "    \n",
    "    return reframe, byteamframe[byteamframe.columns[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline WSE: 0.039\n",
      "Baseline WSE l1: 0.052\n",
      "Baseline MSE: 0.026\n",
      "Baseline MSE l1: 0.035\n",
      "36.36507936507937\n",
      "24.09365079365079\n",
      "34.76825396825397\n"
     ]
    }
   ],
   "source": [
    "print('Baseline WSE: {:.3f}'.format(WSE2(np.array([0]*out[0].shape[0]),out[0])))\n",
    "print('Baseline WSE l1: {:.3f}'.format(WSEl12(np.array([0]*out[0].shape[0]),out[0])))\n",
    "print('Baseline MSE: {:.3f}'.format(WSE2(np.array([0]*out[0].shape[0]),out[0], a=1, b=1)))\n",
    "print('Baseline MSE l1: {:.3f}'.format(WSEl12(np.array([0]*out[0].shape[0]),out[0], a=1, b=1)))\n",
    "\n",
    "print(np.abs(out[1]-out[10]).sum())\n",
    "print(np.abs(out[50]-out[60]).sum())\n",
    "print(np.abs(out[100]-out[110]).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_hopt(config, num_samples=10):#, gpus_per_trial=2):\n",
    "    \n",
    "    trials = Trials()\n",
    "    result = fmin(\n",
    "            fn=train,\n",
    "            space=config,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=num_samples,\n",
    "            trials=trials,\n",
    "            show_progressbar=True),\n",
    "            #early_stop_fn=10,\n",
    "            #trials_save_file=None)\n",
    "    \n",
    "    \n",
    "    return trials\n",
    "    #return best_trained_model\n",
    "    #test_acc = test_accuracy(best_trained_model, device)\n",
    "    #print(\"Best trial test set accuracy: {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'config = {\\n       \"num_layers\": hp.choice(\\'num_layers\\', [1, 2, 3]),\\n       \"num_nodes\": inp.shape[1],\\n       \"scaling_factor\": hp.uniform(\\'scaling_factor\\', 0.5, 1.5),\\n       \"num_nodes_out\": out.shape[1], \\n       \"final_activation\" : None, #hp.choice(\\'final_activation\\',[torch.tanh, None]),\\n       \"clip\": False, #hp.choice(\\'clip\\',[True, False]),\\n       \"batch_size\": 32, #[16, 32, 64, 128]\\n       \"loss_f\": nn.MSELoss(),#hp.choice(\\'loss_f\\',[WSE, nn.MSELoss()]), #, nn.L1Loss()\\n       \"optim\": hp.choice(\\'optim\\',[\\'adam\\', \\'adagrad\\']),\\n       \"lr\": hp.loguniform(\\'lr\\', np.exp(np.log(1e-4)), np.exp(np.log(1e-1))),\\n       \"batch_norm\": hp.choice(\\'batch_norm\\',[True, False]),\\n       \"dropout\": hp.choice(\\'dropout\\',[0.0,0.1,0.2,0.3,0.4,0.5]),#hp.sample_from(lambda _: np.random.uniform(low=0.0, high=.6)),\\n       \"shuffle\": True,\\n       \"num_workers\": 4,\\n       \"patience\": 10,\\n       \"epochs\": 100\\n   }\\n   \\n\\n\\nbtm = run_hopt(config, num_samples=100)\\n#https://docs.ray.io/en/latest/tune/api_docs/suggestion.html#tune-search-alg\\n\\n\\nresults_df=[]\\n\\nfor trial in btm.trials:\\n   results_df.append([trial[\\'result\\'][\\'loss\\'],\\n   [1,2,3][trial[\\'misc\\'][\\'vals\\'][\"num_layers\"][0]],\\n   #[inp.shape[1]][trial[\\'misc\\'][\\'vals\\'][\"num_nodes\"][0]],\\n   trial[\\'misc\\'][\\'vals\\'][\"scaling_factor\"][0],\\n   #[out.shape[1]][trial[\\'misc\\'][\\'vals\\'][\"num_nodes_out\"][0]],\\n   #[torch.tanh, None][trial[\\'misc\\'][\\'vals\\'][\"final_activation\"][0]],\\n   #[True, False][trial[\\'misc\\'][\\'vals\\'][\"clip\"][0]],\\n   #[16, 32, 64, 128][trial[\\'misc\\'][\\'vals\\'][\"batch_size\"][0]],\\n   #trial[\\'misc\\'][\\'vals\\'][\"loss_f\"][0],\\n   [\\'adam\\', \\'adagrad\\'][trial[\\'misc\\'][\\'vals\\'][\"optim\"][0]],\\n   trial[\\'misc\\'][\\'vals\\'][\"lr\"][0],\\n   [True, False][trial[\\'misc\\'][\\'vals\\'][\"batch_norm\"][0]],\\n   [0.0,0.1,0.2,0.3,0.4,0.5][trial[\\'misc\\'][\\'vals\\'][\"dropout\"][0]],\\n   #True][trial[\\'misc\\'][\\'vals\\'][\"shuffle\"][0]],\\n   #[4][trial[\\'misc\\'][\\'vals\\'][\"num_workers\"][0]],\\n   #[10][trial[\\'misc\\'][\\'vals\\'][\"patience\"][0]],\\n   #[50][trial[\\'misc\\'][\\'vals\\'][\"epochs\"][0]]])\\n                      \\n   ])\\n\\n   \\nresults_df=pd.DataFrame(results_df,columns=[\\'loss\\',\\'num_layers\\',\\'scaling_factor\\',#\\'final_activation\\',\\'clip\\',\\n                                \\'optim\\',\\'lr\\',\\'batch_norm\\',\\'dropout\\']).sort_values(\\'loss\\')\\nresults_df.to_hdf(r\\'/Users/federico comitani/GitHub/sodakick/data/hp_res1.h5\\',key=\\'df\\')'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " \"\"\"config = {\n",
    "        \"num_layers\": hp.choice('num_layers', [1, 2, 3]),\n",
    "        \"num_nodes\": inp.shape[1],\n",
    "        \"scaling_factor\": hp.uniform('scaling_factor', 0.5, 1.5),\n",
    "        \"num_nodes_out\": out.shape[1], \n",
    "        \"final_activation\" : None, #hp.choice('final_activation',[torch.tanh, None]),\n",
    "        \"clip\": False, #hp.choice('clip',[True, False]),\n",
    "        \"batch_size\": 32, #[16, 32, 64, 128]\n",
    "        \"loss_f\": nn.MSELoss(),#hp.choice('loss_f',[WSE, nn.MSELoss()]), #, nn.L1Loss()\n",
    "        \"optim\": hp.choice('optim',['adam', 'adagrad']),\n",
    "        \"lr\": hp.loguniform('lr', np.exp(np.log(1e-4)), np.exp(np.log(1e-1))),\n",
    "        \"batch_norm\": hp.choice('batch_norm',[True, False]),\n",
    "        \"dropout\": hp.choice('dropout',[0.0,0.1,0.2,0.3,0.4,0.5]),#hp.sample_from(lambda _: np.random.uniform(low=0.0, high=.6)),\n",
    "        \"shuffle\": True,\n",
    "        \"num_workers\": 4,\n",
    "        \"patience\": 10,\n",
    "        \"epochs\": 100\n",
    "    }\n",
    "    \n",
    "\n",
    "\n",
    "btm = run_hopt(config, num_samples=100)\n",
    "#https://docs.ray.io/en/latest/tune/api_docs/suggestion.html#tune-search-alg\n",
    "\n",
    "\n",
    "results_df=[]\n",
    "\n",
    "for trial in btm.trials:\n",
    "    results_df.append([trial['result']['loss'],\n",
    "    [1,2,3][trial['misc']['vals'][\"num_layers\"][0]],\n",
    "    #[inp.shape[1]][trial['misc']['vals'][\"num_nodes\"][0]],\n",
    "    trial['misc']['vals'][\"scaling_factor\"][0],\n",
    "    #[out.shape[1]][trial['misc']['vals'][\"num_nodes_out\"][0]],\n",
    "    #[torch.tanh, None][trial['misc']['vals'][\"final_activation\"][0]],\n",
    "    #[True, False][trial['misc']['vals'][\"clip\"][0]],\n",
    "    #[16, 32, 64, 128][trial['misc']['vals'][\"batch_size\"][0]],\n",
    "    #trial['misc']['vals'][\"loss_f\"][0],\n",
    "    ['adam', 'adagrad'][trial['misc']['vals'][\"optim\"][0]],\n",
    "    trial['misc']['vals'][\"lr\"][0],\n",
    "    [True, False][trial['misc']['vals'][\"batch_norm\"][0]],\n",
    "    [0.0,0.1,0.2,0.3,0.4,0.5][trial['misc']['vals'][\"dropout\"][0]],\n",
    "    #True][trial['misc']['vals'][\"shuffle\"][0]],\n",
    "    #[4][trial['misc']['vals'][\"num_workers\"][0]],\n",
    "    #[10][trial['misc']['vals'][\"patience\"][0]],\n",
    "    #[50][trial['misc']['vals'][\"epochs\"][0]]])\n",
    "                       \n",
    "    ])\n",
    "\n",
    "    \n",
    "results_df=pd.DataFrame(results_df,columns=['loss','num_layers','scaling_factor',#'final_activation','clip',\n",
    "                                 'optim','lr','batch_norm','dropout']).sort_values('loss')\n",
    "results_df.to_hdf(r'/Users/federico comitani/GitHub/sodakick/data/hp_res1.h5',key='df')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' config = {\\n        \"num_layers\": hp.choice(\\'num_layers\\', [1, 2, 3]),\\n        \"num_nodes\": inp.shape[1],\\n        \"scaling_factor\": hp.uniform(\\'scaling_factor\\', 0.5, 1.5),\\n        \"num_nodes_out\": out.shape[1], \\n        \"final_activation\" : None, #hp.choice(\\'final_activation\\',[torch.tanh, None]),\\n        \"clip\": False, #hp.choice(\\'clip\\',[True, False]),\\n        \"batch_size\": 32, #[16, 32, 64, 128]\\n        \"loss_f\": WSE,#hp.choice(\\'loss_f\\',[WSE, nn.MSELoss()]), #, nn.L1Loss()\\n        \"optim\": hp.choice(\\'optim\\',[\\'adam\\', \\'adagrad\\']),\\n        \"lr\": hp.loguniform(\\'lr\\', np.exp(np.log(1e-4)), np.exp(np.log(1e-1))),\\n        \"batch_norm\": hp.choice(\\'batch_norm\\',[True, False]),\\n        \"dropout\": hp.choice(\\'dropout\\',[0.0,0.1,0.2,0.3,0.4,0.5]),#hp.sample_from(lambda _: np.random.uniform(low=0.0, high=.6)),\\n        \"shuffle\": True,\\n        \"num_workers\": 4,\\n        \"patience\": 10,\\n        \"epochs\": 100\\n    }\\n    \\nbtm = run_hopt(config, num_samples=100)\\n\\nresults_df=[]\\n\\nfor trial in btm.trials:\\n    results_df.append([trial[\\'result\\'][\\'loss\\'],\\n    [1,2,3][trial[\\'misc\\'][\\'vals\\'][\"num_layers\"][0]],\\n    #[inp.shape[1]][trial[\\'misc\\'][\\'vals\\'][\"num_nodes\"][0]],\\n    trial[\\'misc\\'][\\'vals\\'][\"scaling_factor\"][0],\\n    #[out.shape[1]][trial[\\'misc\\'][\\'vals\\'][\"num_nodes_out\"][0]],\\n    #[torch.tanh, None][trial[\\'misc\\'][\\'vals\\'][\"final_activation\"][0]],\\n    #[True, False][trial[\\'misc\\'][\\'vals\\'][\"clip\"][0]],\\n    #[16, 32, 64, 128][trial[\\'misc\\'][\\'vals\\'][\"batch_size\"][0]],\\n    #trial[\\'misc\\'][\\'vals\\'][\"loss_f\"][0],\\n    [\\'adam\\', \\'adagrad\\'][trial[\\'misc\\'][\\'vals\\'][\"optim\"][0]],\\n    trial[\\'misc\\'][\\'vals\\'][\"lr\"][0],\\n    [True, False][trial[\\'misc\\'][\\'vals\\'][\"batch_norm\"][0]],\\n    [0.0,0.1,0.2,0.3,0.4,0.5][trial[\\'misc\\'][\\'vals\\'][\"dropout\"][0]],\\n    #True][trial[\\'misc\\'][\\'vals\\'][\"shuffle\"][0]],\\n    #[4][trial[\\'misc\\'][\\'vals\\'][\"num_workers\"][0]],\\n    #[10][trial[\\'misc\\'][\\'vals\\'][\"patience\"][0]],\\n    #[50][trial[\\'misc\\'][\\'vals\\'][\"epochs\"][0]]])\\n                       \\n    ])\\n\\n    \\nresults_df=pd.DataFrame(results_df,columns=[\\'loss\\',\\'num_layers\\',\\'scaling_factor\\',#\\'final_activation\\',\\'clip\\',\\n                                 \\'optim\\',\\'lr\\',\\'batch_norm\\',\\'dropout\\']).sort_values(\\'loss\\')\\nresults_df.to_hdf(r\\'/Users/federico comitani/GitHub/sodakick/data/hp_res1_wse.h5\\',key=\\'df\\')'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" config = {\n",
    "        \"num_layers\": hp.choice('num_layers', [1, 2, 3]),\n",
    "        \"num_nodes\": inp.shape[1],\n",
    "        \"scaling_factor\": hp.uniform('scaling_factor', 0.5, 1.5),\n",
    "        \"num_nodes_out\": out.shape[1], \n",
    "        \"final_activation\" : None, #hp.choice('final_activation',[torch.tanh, None]),\n",
    "        \"clip\": False, #hp.choice('clip',[True, False]),\n",
    "        \"batch_size\": 32, #[16, 32, 64, 128]\n",
    "        \"loss_f\": WSE,#hp.choice('loss_f',[WSE, nn.MSELoss()]), #, nn.L1Loss()\n",
    "        \"optim\": hp.choice('optim',['adam', 'adagrad']),\n",
    "        \"lr\": hp.loguniform('lr', np.exp(np.log(1e-4)), np.exp(np.log(1e-1))),\n",
    "        \"batch_norm\": hp.choice('batch_norm',[True, False]),\n",
    "        \"dropout\": hp.choice('dropout',[0.0,0.1,0.2,0.3,0.4,0.5]),#hp.sample_from(lambda _: np.random.uniform(low=0.0, high=.6)),\n",
    "        \"shuffle\": True,\n",
    "        \"num_workers\": 4,\n",
    "        \"patience\": 10,\n",
    "        \"epochs\": 100\n",
    "    }\n",
    "    \n",
    "btm = run_hopt(config, num_samples=100)\n",
    "\n",
    "results_df=[]\n",
    "\n",
    "for trial in btm.trials:\n",
    "    results_df.append([trial['result']['loss'],\n",
    "    [1,2,3][trial['misc']['vals'][\"num_layers\"][0]],\n",
    "    #[inp.shape[1]][trial['misc']['vals'][\"num_nodes\"][0]],\n",
    "    trial['misc']['vals'][\"scaling_factor\"][0],\n",
    "    #[out.shape[1]][trial['misc']['vals'][\"num_nodes_out\"][0]],\n",
    "    #[torch.tanh, None][trial['misc']['vals'][\"final_activation\"][0]],\n",
    "    #[True, False][trial['misc']['vals'][\"clip\"][0]],\n",
    "    #[16, 32, 64, 128][trial['misc']['vals'][\"batch_size\"][0]],\n",
    "    #trial['misc']['vals'][\"loss_f\"][0],\n",
    "    ['adam', 'adagrad'][trial['misc']['vals'][\"optim\"][0]],\n",
    "    trial['misc']['vals'][\"lr\"][0],\n",
    "    [True, False][trial['misc']['vals'][\"batch_norm\"][0]],\n",
    "    [0.0,0.1,0.2,0.3,0.4,0.5][trial['misc']['vals'][\"dropout\"][0]],\n",
    "    #True][trial['misc']['vals'][\"shuffle\"][0]],\n",
    "    #[4][trial['misc']['vals'][\"num_workers\"][0]],\n",
    "    #[10][trial['misc']['vals'][\"patience\"][0]],\n",
    "    #[50][trial['misc']['vals'][\"epochs\"][0]]])\n",
    "                       \n",
    "    ])\n",
    "\n",
    "    \n",
    "results_df=pd.DataFrame(results_df,columns=['loss','num_layers','scaling_factor',#'final_activation','clip',\n",
    "                                 'optim','lr','batch_norm','dropout']).sort_values('loss')\n",
    "results_df.to_hdf(r'/Users/federico comitani/GitHub/sodakick/data/hp_res1_wse.h5',key='df')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [2:47:10<00:00, 200.61s/trial, best loss: 0.007322522163844834]   \n"
     ]
    }
   ],
   "source": [
    " config = {\n",
    "        \"num_layers\": hp.choice('num_layers', [1, 2, 3]),\n",
    "        \"num_nodes\": inp.shape[1],\n",
    "        \"scaling_factor\": hp.uniform('scaling_factor', 0.5, 1.5),\n",
    "        \"num_nodes_out\": out.shape[1], \n",
    "        \"final_activation\" : hp.choice('final_activation',[torch.tanh, None]),\n",
    "        \"clip\": False, #hp.choice('clip',[True, False]),\n",
    "        \"batch_size\": 32, #[16, 32, 64, 128]\n",
    "        \"loss_f\": nn.MSELoss(),#WSE,#hp.choice('loss_f',[WSE, nn.MSELoss()]), #, nn.L1Loss()\n",
    "        \"optim\": 'adam',#hp.choice('optim',['adam', 'adagrad']),\n",
    "        \"lr\": 0.0001,#hp.choice('lr',[0.0001,0.001,.00001]),#hp.loguniform('lr', np.exp(np.log(1e-4)), np.exp(np.log(1e-1))),\n",
    "        \"batch_norm\": False,#hp.choice('batch_norm',[True, False]),\n",
    "        \"dropout\": 0.0,#hp.choice('dropout',[0.0,0.1,0.2,0.3]),#hp.sample_from(lambda _: np.random.uniform(low=0.0, high=.6)),\n",
    "        \"shuffle\": True,\n",
    "        \"num_workers\": 4,\n",
    "        \"patience\": 10,\n",
    "        \"epochs\": 100\n",
    "    }\n",
    "    \n",
    "btm = run_hopt(config, num_samples=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//miniconda3/lib/python3.7/site-packages/pandas/core/generic.py:2621: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['final_activation'], dtype='object')]\n",
      "\n",
      "  encoding=encoding,\n"
     ]
    }
   ],
   "source": [
    "results_df=[]\n",
    "\n",
    "for trial in btm.trials:\n",
    "    results_df.append([trial['result']['loss'],\n",
    "    trial['result']['mse'],\n",
    "    [1,2,3][trial['misc']['vals'][\"num_layers\"][0]],\n",
    "    #[inp.shape[1]][trial['misc']['vals'][\"num_nodes\"][0]],\n",
    "    trial['misc']['vals'][\"scaling_factor\"][0],\n",
    "    #[out.shape[1]][trial['misc']['vals'][\"num_nodes_out\"][0]],\n",
    "    [torch.tanh, None][trial['misc']['vals'][\"final_activation\"][0]],\n",
    "    #[True, False][trial['misc']['vals'][\"clip\"][0]],\n",
    "    #[16, 32, 64, 128][trial['misc']['vals'][\"batch_size\"][0]],\n",
    "    #trial['misc']['vals'][\"loss_f\"][0],\n",
    "    #['adam', 'adagrad'][trial['misc']['vals'][\"optim\"][0]],\n",
    "    #[0.0001,0.001,.01,.1][trial['misc']['vals'][\"lr\"][0]],\n",
    "    #[True, False][trial['misc']['vals'][\"batch_norm\"][0]],\n",
    "    #[0.0,0.1,0.2,0.3,0.4,0.5][trial['misc']['vals'][\"dropout\"][0]],\n",
    "    #True][trial['misc']['vals'][\"shuffle\"][0]],\n",
    "    #[4][trial['misc']['vals'][\"num_workers\"][0]],\n",
    "    #[10][trial['misc']['vals'][\"patience\"][0]],\n",
    "    #[50][trial['misc']['vals'][\"epochs\"][0]]])\n",
    "                       \n",
    "    ])\n",
    "\n",
    "\n",
    "results_df=pd.DataFrame(results_df,columns=['loss',\n",
    "                                            'mse',\n",
    "                                            'num_layers',\n",
    "                                            'scaling_factor',\n",
    "                                            'final_activation',\n",
    "                                            #'clip', \n",
    "                                            #'optim', \n",
    "                                            #'lr',\n",
    "                                            #'batch_norm',\n",
    "                                            #'dropout'\n",
    "                                            ]).sort_values('loss')\n",
    "\n",
    "results_df.to_hdf(r'/Users/federico comitani/GitHub/sodakick/data/hp_res1_mse_avg.h5',key='df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>mse</th>\n",
       "      <th>num_layers</th>\n",
       "      <th>scaling_factor</th>\n",
       "      <th>final_activation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.007323</td>\n",
       "      <td>0.007323</td>\n",
       "      <td>1</td>\n",
       "      <td>1.496729</td>\n",
       "      <td>&lt;built-in method tanh of type object at 0x13b7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.007334</td>\n",
       "      <td>0.007334</td>\n",
       "      <td>1</td>\n",
       "      <td>1.409198</td>\n",
       "      <td>&lt;built-in method tanh of type object at 0x13b7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.007341</td>\n",
       "      <td>0.007341</td>\n",
       "      <td>1</td>\n",
       "      <td>1.434899</td>\n",
       "      <td>&lt;built-in method tanh of type object at 0x13b7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.007344</td>\n",
       "      <td>0.007344</td>\n",
       "      <td>1</td>\n",
       "      <td>1.449452</td>\n",
       "      <td>&lt;built-in method tanh of type object at 0x13b7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.007345</td>\n",
       "      <td>0.007345</td>\n",
       "      <td>1</td>\n",
       "      <td>1.408989</td>\n",
       "      <td>&lt;built-in method tanh of type object at 0x13b7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.007345</td>\n",
       "      <td>0.007345</td>\n",
       "      <td>1</td>\n",
       "      <td>1.377020</td>\n",
       "      <td>&lt;built-in method tanh of type object at 0x13b7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.007345</td>\n",
       "      <td>0.007345</td>\n",
       "      <td>1</td>\n",
       "      <td>1.496375</td>\n",
       "      <td>&lt;built-in method tanh of type object at 0x13b7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.007351</td>\n",
       "      <td>0.007351</td>\n",
       "      <td>1</td>\n",
       "      <td>1.315450</td>\n",
       "      <td>&lt;built-in method tanh of type object at 0x13b7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.007354</td>\n",
       "      <td>0.007354</td>\n",
       "      <td>1</td>\n",
       "      <td>1.449485</td>\n",
       "      <td>&lt;built-in method tanh of type object at 0x13b7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.007359</td>\n",
       "      <td>0.007359</td>\n",
       "      <td>1</td>\n",
       "      <td>1.388184</td>\n",
       "      <td>&lt;built-in method tanh of type object at 0x13b7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.007369</td>\n",
       "      <td>0.007369</td>\n",
       "      <td>2</td>\n",
       "      <td>1.483638</td>\n",
       "      <td>&lt;built-in method tanh of type object at 0x13b7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.007370</td>\n",
       "      <td>0.007370</td>\n",
       "      <td>1</td>\n",
       "      <td>1.493102</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.007372</td>\n",
       "      <td>0.007372</td>\n",
       "      <td>1</td>\n",
       "      <td>1.344872</td>\n",
       "      <td>&lt;built-in method tanh of type object at 0x13b7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.007379</td>\n",
       "      <td>0.007379</td>\n",
       "      <td>1</td>\n",
       "      <td>1.201574</td>\n",
       "      <td>&lt;built-in method tanh of type object at 0x13b7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.007379</td>\n",
       "      <td>0.007379</td>\n",
       "      <td>2</td>\n",
       "      <td>1.495603</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.007382</td>\n",
       "      <td>0.007382</td>\n",
       "      <td>1</td>\n",
       "      <td>1.340519</td>\n",
       "      <td>&lt;built-in method tanh of type object at 0x13b7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.007384</td>\n",
       "      <td>0.007384</td>\n",
       "      <td>2</td>\n",
       "      <td>1.473994</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.007386</td>\n",
       "      <td>0.007386</td>\n",
       "      <td>2</td>\n",
       "      <td>1.317459</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.007387</td>\n",
       "      <td>0.007387</td>\n",
       "      <td>2</td>\n",
       "      <td>1.480112</td>\n",
       "      <td>&lt;built-in method tanh of type object at 0x13b7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.007393</td>\n",
       "      <td>0.007393</td>\n",
       "      <td>1</td>\n",
       "      <td>1.154967</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.007394</td>\n",
       "      <td>0.007394</td>\n",
       "      <td>1</td>\n",
       "      <td>1.237638</td>\n",
       "      <td>&lt;built-in method tanh of type object at 0x13b7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.007397</td>\n",
       "      <td>0.007397</td>\n",
       "      <td>1</td>\n",
       "      <td>1.244092</td>\n",
       "      <td>&lt;built-in method tanh of type object at 0x13b7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.007399</td>\n",
       "      <td>0.007399</td>\n",
       "      <td>2</td>\n",
       "      <td>1.431953</td>\n",
       "      <td>&lt;built-in method tanh of type object at 0x13b7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.007406</td>\n",
       "      <td>0.007406</td>\n",
       "      <td>1</td>\n",
       "      <td>1.191060</td>\n",
       "      <td>&lt;built-in method tanh of type object at 0x13b7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.007414</td>\n",
       "      <td>0.007414</td>\n",
       "      <td>1</td>\n",
       "      <td>1.172791</td>\n",
       "      <td>&lt;built-in method tanh of type object at 0x13b7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.007423</td>\n",
       "      <td>0.007423</td>\n",
       "      <td>2</td>\n",
       "      <td>1.364584</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.007432</td>\n",
       "      <td>0.007432</td>\n",
       "      <td>2</td>\n",
       "      <td>1.293048</td>\n",
       "      <td>&lt;built-in method tanh of type object at 0x13b7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.007434</td>\n",
       "      <td>0.007434</td>\n",
       "      <td>1</td>\n",
       "      <td>0.927668</td>\n",
       "      <td>&lt;built-in method tanh of type object at 0x13b7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.007434</td>\n",
       "      <td>0.007434</td>\n",
       "      <td>1</td>\n",
       "      <td>1.037191</td>\n",
       "      <td>&lt;built-in method tanh of type object at 0x13b7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.007436</td>\n",
       "      <td>0.007436</td>\n",
       "      <td>1</td>\n",
       "      <td>0.943016</td>\n",
       "      <td>&lt;built-in method tanh of type object at 0x13b7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.007439</td>\n",
       "      <td>0.007439</td>\n",
       "      <td>2</td>\n",
       "      <td>1.301468</td>\n",
       "      <td>&lt;built-in method tanh of type object at 0x13b7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.007442</td>\n",
       "      <td>0.007442</td>\n",
       "      <td>1</td>\n",
       "      <td>0.917709</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.007454</td>\n",
       "      <td>0.007454</td>\n",
       "      <td>2</td>\n",
       "      <td>1.295467</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.007457</td>\n",
       "      <td>0.007457</td>\n",
       "      <td>1</td>\n",
       "      <td>1.135130</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.007462</td>\n",
       "      <td>0.007462</td>\n",
       "      <td>1</td>\n",
       "      <td>0.859069</td>\n",
       "      <td>&lt;built-in method tanh of type object at 0x13b7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.007474</td>\n",
       "      <td>0.007474</td>\n",
       "      <td>1</td>\n",
       "      <td>1.034647</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.007487</td>\n",
       "      <td>0.007487</td>\n",
       "      <td>1</td>\n",
       "      <td>0.841959</td>\n",
       "      <td>&lt;built-in method tanh of type object at 0x13b7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.007523</td>\n",
       "      <td>0.007523</td>\n",
       "      <td>1</td>\n",
       "      <td>0.743721</td>\n",
       "      <td>&lt;built-in method tanh of type object at 0x13b7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.007527</td>\n",
       "      <td>0.007527</td>\n",
       "      <td>1</td>\n",
       "      <td>0.738605</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.007584</td>\n",
       "      <td>0.007584</td>\n",
       "      <td>3</td>\n",
       "      <td>1.290168</td>\n",
       "      <td>&lt;built-in method tanh of type object at 0x13b7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.007617</td>\n",
       "      <td>0.007617</td>\n",
       "      <td>2</td>\n",
       "      <td>1.082287</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.007651</td>\n",
       "      <td>0.007651</td>\n",
       "      <td>3</td>\n",
       "      <td>1.261721</td>\n",
       "      <td>&lt;built-in method tanh of type object at 0x13b7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.007678</td>\n",
       "      <td>0.007678</td>\n",
       "      <td>3</td>\n",
       "      <td>1.123099</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.007682</td>\n",
       "      <td>0.007682</td>\n",
       "      <td>2</td>\n",
       "      <td>0.989746</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.007744</td>\n",
       "      <td>0.007744</td>\n",
       "      <td>3</td>\n",
       "      <td>1.105897</td>\n",
       "      <td>&lt;built-in method tanh of type object at 0x13b7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.008510</td>\n",
       "      <td>0.008510</td>\n",
       "      <td>2</td>\n",
       "      <td>0.641467</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.009588</td>\n",
       "      <td>0.009588</td>\n",
       "      <td>3</td>\n",
       "      <td>0.738355</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.011552</td>\n",
       "      <td>0.011552</td>\n",
       "      <td>3</td>\n",
       "      <td>0.639328</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.014494</td>\n",
       "      <td>0.014494</td>\n",
       "      <td>3</td>\n",
       "      <td>0.539432</td>\n",
       "      <td>&lt;built-in method tanh of type object at 0x13b7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.017301</td>\n",
       "      <td>0.017301</td>\n",
       "      <td>3</td>\n",
       "      <td>0.525561</td>\n",
       "      <td>&lt;built-in method tanh of type object at 0x13b7...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        loss       mse  num_layers  scaling_factor  \\\n",
       "45  0.007323  0.007323           1        1.496729   \n",
       "27  0.007334  0.007334           1        1.409198   \n",
       "25  0.007341  0.007341           1        1.434899   \n",
       "40  0.007344  0.007344           1        1.449452   \n",
       "22  0.007345  0.007345           1        1.408989   \n",
       "43  0.007345  0.007345           1        1.377020   \n",
       "15  0.007345  0.007345           1        1.496375   \n",
       "47  0.007351  0.007351           1        1.315450   \n",
       "34  0.007354  0.007354           1        1.449485   \n",
       "23  0.007359  0.007359           1        1.388184   \n",
       "48  0.007369  0.007369           2        1.483638   \n",
       "5   0.007370  0.007370           1        1.493102   \n",
       "38  0.007372  0.007372           1        1.344872   \n",
       "20  0.007379  0.007379           1        1.201574   \n",
       "4   0.007379  0.007379           2        1.495603   \n",
       "33  0.007382  0.007382           1        1.340519   \n",
       "44  0.007384  0.007384           2        1.473994   \n",
       "6   0.007386  0.007386           2        1.317459   \n",
       "7   0.007387  0.007387           2        1.480112   \n",
       "49  0.007393  0.007393           1        1.154967   \n",
       "24  0.007394  0.007394           1        1.237638   \n",
       "30  0.007397  0.007397           1        1.244092   \n",
       "11  0.007399  0.007399           2        1.431953   \n",
       "26  0.007406  0.007406           1        1.191060   \n",
       "37  0.007414  0.007414           1        1.172791   \n",
       "8   0.007423  0.007423           2        1.364584   \n",
       "16  0.007432  0.007432           2        1.293048   \n",
       "28  0.007434  0.007434           1        0.927668   \n",
       "18  0.007434  0.007434           1        1.037191   \n",
       "31  0.007436  0.007436           1        0.943016   \n",
       "3   0.007439  0.007439           2        1.301468   \n",
       "1   0.007442  0.007442           1        0.917709   \n",
       "0   0.007454  0.007454           2        1.295467   \n",
       "9   0.007457  0.007457           1        1.135130   \n",
       "21  0.007462  0.007462           1        0.859069   \n",
       "12  0.007474  0.007474           1        1.034647   \n",
       "32  0.007487  0.007487           1        0.841959   \n",
       "36  0.007523  0.007523           1        0.743721   \n",
       "2   0.007527  0.007527           1        0.738605   \n",
       "42  0.007584  0.007584           3        1.290168   \n",
       "39  0.007617  0.007617           2        1.082287   \n",
       "35  0.007651  0.007651           3        1.261721   \n",
       "13  0.007678  0.007678           3        1.123099   \n",
       "41  0.007682  0.007682           2        0.989746   \n",
       "29  0.007744  0.007744           3        1.105897   \n",
       "10  0.008510  0.008510           2        0.641467   \n",
       "46  0.009588  0.009588           3        0.738355   \n",
       "19  0.011552  0.011552           3        0.639328   \n",
       "14  0.014494  0.014494           3        0.539432   \n",
       "17  0.017301  0.017301           3        0.525561   \n",
       "\n",
       "                                     final_activation  \n",
       "45  <built-in method tanh of type object at 0x13b7...  \n",
       "27  <built-in method tanh of type object at 0x13b7...  \n",
       "25  <built-in method tanh of type object at 0x13b7...  \n",
       "40  <built-in method tanh of type object at 0x13b7...  \n",
       "22  <built-in method tanh of type object at 0x13b7...  \n",
       "43  <built-in method tanh of type object at 0x13b7...  \n",
       "15  <built-in method tanh of type object at 0x13b7...  \n",
       "47  <built-in method tanh of type object at 0x13b7...  \n",
       "34  <built-in method tanh of type object at 0x13b7...  \n",
       "23  <built-in method tanh of type object at 0x13b7...  \n",
       "48  <built-in method tanh of type object at 0x13b7...  \n",
       "5                                                None  \n",
       "38  <built-in method tanh of type object at 0x13b7...  \n",
       "20  <built-in method tanh of type object at 0x13b7...  \n",
       "4                                                None  \n",
       "33  <built-in method tanh of type object at 0x13b7...  \n",
       "44                                               None  \n",
       "6                                                None  \n",
       "7   <built-in method tanh of type object at 0x13b7...  \n",
       "49                                               None  \n",
       "24  <built-in method tanh of type object at 0x13b7...  \n",
       "30  <built-in method tanh of type object at 0x13b7...  \n",
       "11  <built-in method tanh of type object at 0x13b7...  \n",
       "26  <built-in method tanh of type object at 0x13b7...  \n",
       "37  <built-in method tanh of type object at 0x13b7...  \n",
       "8                                                None  \n",
       "16  <built-in method tanh of type object at 0x13b7...  \n",
       "28  <built-in method tanh of type object at 0x13b7...  \n",
       "18  <built-in method tanh of type object at 0x13b7...  \n",
       "31  <built-in method tanh of type object at 0x13b7...  \n",
       "3   <built-in method tanh of type object at 0x13b7...  \n",
       "1                                                None  \n",
       "0                                                None  \n",
       "9                                                None  \n",
       "21  <built-in method tanh of type object at 0x13b7...  \n",
       "12                                               None  \n",
       "32  <built-in method tanh of type object at 0x13b7...  \n",
       "36  <built-in method tanh of type object at 0x13b7...  \n",
       "2                                                None  \n",
       "42  <built-in method tanh of type object at 0x13b7...  \n",
       "39                                               None  \n",
       "35  <built-in method tanh of type object at 0x13b7...  \n",
       "13                                               None  \n",
       "41                                               None  \n",
       "29  <built-in method tanh of type object at 0x13b7...  \n",
       "10                                               None  \n",
       "46                                               None  \n",
       "19                                               None  \n",
       "14  <built-in method tanh of type object at 0x13b7...  \n",
       "17  <built-in method tanh of type object at 0x13b7...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.sort_values('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_layers': 1,\n",
       " 'num_nodes': 816,\n",
       " 'scaling_factor': 1.496729193918135,\n",
       " 'num_nodes_out': 384,\n",
       " 'final_activation': <function _VariableFunctionsClass.tanh>,\n",
       " 'clip': False,\n",
       " 'batch_size': 32,\n",
       " 'loss_f': MSELoss(),\n",
       " 'optim': 'adam',\n",
       " 'lr': 0.0001,\n",
       " 'batch_norm': False,\n",
       " 'dropout': 0.0,\n",
       " 'shuffle': True,\n",
       " 'num_workers': 4,\n",
       " 'patience': 10,\n",
       " 'epochs': 100}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "conf_final=copy.deepcopy(config)\n",
    "\n",
    "for key,value in results_df.sort_values('loss').iloc[0].to_dict().items():\n",
    "    if key in conf_final:\n",
    "        conf_final[key]=value\n",
    "        \n",
    "conf_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_alone(config, model=Net, silent=True, checkpoint_dir=None):\n",
    "    \n",
    "    \n",
    "    phases = ['train','val']\n",
    "\n",
    "    #x_train, x_test, y_train, y_test = data[0], data[1], data[2], data[3]\n",
    "\n",
    "    training_set = matchesDataset(x_train, y_train)\n",
    "    trainBatch = torch.utils.data.DataLoader(training_set, batch_size=config['batch_size'], shuffle=config['shuffle'], num_workers=config['num_workers'])\n",
    "\n",
    "    validation_set = matchesDataset(x_test, y_test)\n",
    "    valBatch = torch.utils.data.DataLoader(validation_set, batch_size=config['batch_size'], shuffle=config['shuffle'], num_workers=config['num_workers'])\n",
    "\n",
    "    earlStop = EarlyStopping(patience=int(config['patience']), keepBest=True)\n",
    "\n",
    "    net = model(int(config['num_layers']), int(config['num_nodes']), config['scaling_factor'], \n",
    "                int(config['num_nodes_out']), config['final_activation'], config['batch_norm'], config['dropout'])\n",
    "\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            net = nn.DataParallel(net)\n",
    "    net.to(device)\n",
    "\n",
    "    if checkpoint_dir:\n",
    "        model_state, optimizer_state = torch.load(\n",
    "            os.path.join(checkpoint_dir, \"checkpoint\"))\n",
    "        net.load_state_dict(model_state)\n",
    "        optimizer.load_state_dict(optimizer_state)\n",
    "\n",
    "    if config['optim']=='adam':\n",
    "        optimizer = Adam(net.parameters(), lr=config['lr'])\n",
    "    elif config['optim']=='adagrad':\n",
    "        optimizer = Adagrad(net.parameters(), lr=config['lr'])\n",
    "    else:\n",
    "        print('optim error')\n",
    "        return\n",
    "\n",
    "\n",
    "    losses=[[],[]]\n",
    "    mses=[]\n",
    "    diffs=[]\n",
    "    exit=False\n",
    "\n",
    "    for epoch in tqdm(range(config['epochs']), desc='Epoch'):\n",
    "    #for epoch in range(config['epochs']):\n",
    "\n",
    "        if exit:\n",
    "            break\n",
    "\n",
    "        for phase in phases:\n",
    "            if phase == 'train':\n",
    "                net.train(True) \n",
    "\n",
    "                \"\"\" Run the training of the model. \"\"\"    \n",
    "\n",
    "                losses_batch=[]\n",
    "                for batchNum, batch in enumerate(trainBatch):\n",
    "\n",
    "                    x = batch[0]\n",
    "                    y = batch[1]\n",
    "\n",
    "                    \"\"\" Move batches to GPU if available. \"\"\"\n",
    "\n",
    "                    if torch.cuda.is_available():\n",
    "                        x = x.cuda()\n",
    "                        y = y.cuda()\n",
    "\n",
    "                    \"\"\" Core of training. \"\"\"\n",
    "\n",
    "                    loss = config['loss_f'](net(x), y)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    if config['clip']:\n",
    "                        net.clp()\n",
    "\n",
    "                    losses_batch.append(loss)\n",
    "\n",
    "                \"\"\" Early stop check. \"\"\"\n",
    "\n",
    "                earlStop(loss, net)\n",
    "                finalepoch = epoch\n",
    "\n",
    "                if earlStop.earlyStop:\n",
    "\n",
    "                    if not silent:\n",
    "                        print('Limit loss improvement reached, stopping the training.')\n",
    "\n",
    "                    exit=True \n",
    "\n",
    "                #losses[0].append(torch.mean(torch.stack(losses_batch)).detach().cpu().numpy())\n",
    "            else:\n",
    "                net.train(False)\n",
    "                net.eval()\n",
    "\n",
    "                val_loss=0\n",
    "                val_mse=0\n",
    "\n",
    "                losses_batch=[]\n",
    "                for batchNum, batch in enumerate(valBatch):\n",
    "\n",
    "                    x = batch[0]\n",
    "                    y = batch[1]\n",
    "\n",
    "                    \"\"\" Move batches to GPU if available. \"\"\"\n",
    "\n",
    "                    if torch.cuda.is_available():\n",
    "                        x = x.cuda()\n",
    "                        y = y.cuda()\n",
    "\n",
    "                    \"\"\" Core of training. \"\"\"\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    output=net(x)\n",
    "                    target=y\n",
    "                    loss = config['loss_f'](output, target)\n",
    "\n",
    "                    #losses_batch.append(loss)\n",
    "                    val_loss+=loss.detach().numpy()\n",
    "                    val_mse+=nn.MSELoss()(output, target).detach().numpy()\n",
    "\n",
    "                #losses[1].append(torch.mean(torch.stack(losses_batch)).detach().cpu().numpy())\n",
    "\n",
    "                #with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
    "                #    path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "                #    torch.save((net.state_dict(), optimizer.state_dict()), path)\n",
    "\n",
    "                #tune.report(loss=(val_loss/batchNum), mse=(val_mse/batchNum))\n",
    "                #tune.report(loss=torch.mean(torch.stack(losses_batch)))\n",
    "\n",
    "    return net, val_loss/batchNum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  27%|██▋       | 27/100 [02:38<07:09,  5.88s/it]\n"
     ]
    }
   ],
   "source": [
    "net,loss=train_alone(conf_final, model=Net, silent=True, checkpoint_dir=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=net(torch.Tensor(inp)).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   goals  assists  cards_yellow  cards_red  own_goals  goals_against  saves\n",
      "0      2        1             2          0          0              1      3\n",
      "1      1        0             2          0          0              2      4\n",
      "   goals  assists  cards_yellow  cards_red  own_goals  goals_against  saves\n",
      "0      1        1             4          1          0              0      2\n",
      "1      0        0             1          0          0              1      1\n"
     ]
    }
   ],
   "source": [
    "i=1000\n",
    "cats=['minutes','goals','assists','cards_yellow','cards_red','own_goals']+['goals_against','saves']\n",
    "\n",
    "reframe, byteamframe = revert_output(pred[i])\n",
    "print(byteamframe.astype(int))\n",
    "reframe, byteamframe = revert_output(out[i])\n",
    "print(byteamframe.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
